{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:37.830856Z",
     "start_time": "2020-11-02T00:58:37.351914Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from thop import profile\n",
    "import json\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:37.857881Z",
     "start_time": "2020-11-02T00:58:37.831857Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:37.860883Z",
     "start_time": "2020-11-02T00:58:37.858881Z"
    }
   },
   "outputs": [],
   "source": [
    "dataSet = \"cifar\"\n",
    "data_dir = \"./data/teaLevel/singleLeaf/\"\n",
    "num_classes = 10\n",
    "batch_size=  256\n",
    "arch = \"mobilenetV2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:39.352753Z",
     "start_time": "2020-11-02T00:58:37.861884Z"
    },
    "code_folding": [
     1,
     16
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing data..')\n",
    "if dataSet == \"cifar\":\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR100(root='./data/cifar-100/', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR100(root='./data/cifar-100/', train=False, download=True, transform=transform_test)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True,drop_last=False)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False,drop_last=False)\n",
    "if dataSet == \"tea\":\n",
    "    print('load training data')\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])]),\n",
    "        \"val\": transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])}\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=data_dir+\"train\",transform=data_transform[\"train\"])\n",
    "    train_num = len(train_dataset)\n",
    "\n",
    "    tea_list = train_dataset.class_to_idx\n",
    "    cla_dict = dict((val, key) for key, val in tea_list.items())\n",
    "    json_str = json.dumps(cla_dict, indent=4)\n",
    "    with open('class_indices.json', 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=0)\n",
    "\n",
    "    validate_dataset = datasets.ImageFolder(root=data_dir + \"val\",transform=data_transform[\"val\"])\n",
    "    val_num = len(validate_dataset)\n",
    "    test_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:39.371770Z",
     "start_time": "2020-11-02T00:58:39.353754Z"
    },
    "code_folding": [
     3,
     9,
     15,
     18,
     58
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building mobileNetV2 model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Building mobileNetV2 model..')\n",
    "import math\n",
    "import pdb\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "def make_divisible(x, divisible_by=8):\n",
    "    import numpy as np\n",
    "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, compress_rate, n_class, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t-ex, c-channel, n-blocknum, s-stride\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 1], # NOTE: change stride 2 -> 1 for CIFAR10\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "        self.compress_rate=compress_rate[:]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        # input_channel = make_divisible(input_channel * width_mult)  # first channel is always 32!\n",
    "        self.last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        cnt=1\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = make_divisible(c * width_mult) if t > 1 else c\n",
    "            output_channel = int((1-self.compress_rate[cnt])*output_channel)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "            cnt+=1\n",
    "\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        #self.classifier = nn.Linear(self.last_channel, n_class)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "def mobilenet_v2(compress_rate,n_class=10):\n",
    "    model = MobileNetV2(compress_rate=compress_rate,n_class=n_class,width_mult=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:39.394791Z",
     "start_time": "2020-11-02T00:58:39.372771Z"
    },
    "code_folding": [
     1,
     27,
     31,
     34,
     41,
     88
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare resNet_50 model...\n"
     ]
    }
   ],
   "source": [
    "print(\"prepare resNet_50 model...\")\n",
    "def adapt_channel(compress_rate, num_layers):\n",
    "\n",
    "    if num_layers==56:\n",
    "        stage_repeat = [9, 9, 9]\n",
    "        stage_out_channel = [16] + [16] * 9 + [32] * 9 + [64] * 9\n",
    "    elif num_layers==110:\n",
    "        stage_repeat = [18, 18, 18]\n",
    "        stage_out_channel = [16] + [16] * 18 + [32] * 18 + [64] * 18\n",
    "\n",
    "    stage_oup_cprate = []\n",
    "    stage_oup_cprate += [compress_rate[0]]\n",
    "    for i in range(len(stage_repeat)-1):\n",
    "        stage_oup_cprate += [compress_rate[i+1]] * stage_repeat[i]\n",
    "    stage_oup_cprate +=[0.] * stage_repeat[-1]\n",
    "    mid_cprate = compress_rate[len(stage_repeat):]\n",
    "\n",
    "    overall_channel = []\n",
    "    mid_channel = []\n",
    "    for i in range(len(stage_out_channel)):\n",
    "        if i == 0 :\n",
    "            overall_channel += [int(stage_out_channel[i] * (1-stage_oup_cprate[i]))]\n",
    "        else:\n",
    "            overall_channel += [int(stage_out_channel[i] * (1-stage_oup_cprate[i]))]\n",
    "            mid_channel += [int(stage_out_channel[i] * (1-mid_cprate[i-1]))]\n",
    "\n",
    "    return overall_channel, mid_channel\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, midplanes, inplanes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.planes = planes\n",
    "        self.conv1 = conv3x3(inplanes, midplanes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(midplanes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = conv3x3(midplanes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.stride = stride\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inplanes != planes:\n",
    "            if stride!=1:\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(x[:, :, ::2, ::2],\n",
    "                                    (0, 0, 0, 0, (planes-inplanes)//2, planes-inplanes-(planes-inplanes)//2), \"constant\", 0))\n",
    "            else:\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(x[:, :, :, :],\n",
    "                                    (0, 0, 0, 0, (planes-inplanes)//2, planes-inplanes-(planes-inplanes)//2), \"constant\", 0))\n",
    "            #self.shortcut = LambdaLayer(\n",
    "            #    lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4),\"constant\", 0))\n",
    "\n",
    "            '''self.shortcut = nn.Sequential(\n",
    "                conv1x1(inplanes, planes, stride=stride),\n",
    "                #nn.BatchNorm2d(planes),\n",
    "            )#'''\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        #print(self.stride, self.inplanes, self.planes, out.size(), self.shortcut(x).size())\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_layers, compress_rate, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert (num_layers - 2) % 6 == 0, 'depth should be 6n+2'\n",
    "        n = (num_layers - 2) // 6\n",
    "\n",
    "        self.num_layer = num_layers\n",
    "        self.overall_channel, self.mid_channel = adapt_channel(compress_rate, num_layers)\n",
    "\n",
    "        self.layer_num = 0\n",
    "        self.conv1 = nn.Conv2d(3, self.overall_channel[self.layer_num], kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.overall_channel[self.layer_num])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_num += 1\n",
    "\n",
    "        #self.layers = nn.ModuleList()\n",
    "        self.layer1 = self._make_layer(block, blocks_num=n, stride=1)\n",
    "        self.layer2 = self._make_layer(block, blocks_num=n, stride=2)\n",
    "        self.layer3 = self._make_layer(block, blocks_num=n, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        if self.num_layer == 56:\n",
    "            self.fc = nn.Linear(64 * BasicBlock.expansion, num_classes)\n",
    "        else:\n",
    "            self.linear = nn.Linear(64 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, blocks_num, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.mid_channel[self.layer_num - 1], self.overall_channel[self.layer_num - 1],\n",
    "                                 self.overall_channel[self.layer_num], stride))\n",
    "        self.layer_num += 1\n",
    "\n",
    "        for i in range(1, blocks_num):\n",
    "            layers.append(block(self.mid_channel[self.layer_num - 1], self.overall_channel[self.layer_num - 1],\n",
    "                                     self.overall_channel[self.layer_num]))\n",
    "            self.layer_num += 1\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        for i, block in enumerate(self.layer1):\n",
    "            x = block(x)\n",
    "        for i, block in enumerate(self.layer2):\n",
    "            x = block(x)\n",
    "        for i, block in enumerate(self.layer3):\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.num_layer == 56:\n",
    "            x = self.fc(x)\n",
    "        else:\n",
    "            x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resnet_56(compress_rate,num_classes=9):\n",
    "    return ResNet(BasicBlock, 56, compress_rate,num_classes)\n",
    "\n",
    "def resnet_110(compress_rate,num_classes=9):\n",
    "    return ResNet(BasicBlock, 110, compress_rate,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:39.405801Z",
     "start_time": "2020-11-02T00:58:39.395793Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare vgg...\n"
     ]
    }
   ],
   "source": [
    "print(\"prepare vgg...\")\n",
    "defaultcfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 8192, 512]\n",
    "relucfg = [2, 6, 9, 13, 16, 19, 23, 26, 29, 33, 36, 39]\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, compress_rate, cfg=None, num_classes=9):\n",
    "        super(VGG, self).__init__()\n",
    "\n",
    "        if cfg is None:\n",
    "            cfg = defaultcfg\n",
    "        self.relucfg = relucfg\n",
    "\n",
    "        self.compress_rate = compress_rate[:]\n",
    "        self.compress_rate.append(0.0)\n",
    "\n",
    "        self.features = self._make_layers(cfg)\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(cfg[-2], cfg[-1])),\n",
    "            ('norm1', nn.BatchNorm1d(cfg[-1])),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('linear2', nn.Linear(cfg[-1], num_classes)),\n",
    "        ]))\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "\n",
    "        layers = nn.Sequential()\n",
    "        in_channels = 3\n",
    "        cnt=0\n",
    "\n",
    "        for i, x in enumerate(cfg):\n",
    "            if x == 'M':\n",
    "                layers.add_module('pool%d' % i, nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            else:\n",
    "                x = int(x * (1-self.compress_rate[cnt]))\n",
    "\n",
    "                cnt+=1\n",
    "                conv2d = nn.Conv2d(in_channels, x, kernel_size=3, padding=1)\n",
    "                layers.add_module('conv%d' % i, conv2d)\n",
    "                layers.add_module('norm%d' % i, nn.BatchNorm2d(x))\n",
    "                layers.add_module('relu%d' % i, nn.ReLU(inplace=True))\n",
    "                in_channels = x\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = nn.AvgPool2d(2)(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x  \n",
    "def vgg_16_bn(compress_rate):\n",
    "    return VGG(compress_rate=compress_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:39.425820Z",
     "start_time": "2020-11-02T00:58:39.406802Z"
    },
    "code_folding": [
     3,
     24,
     38,
     111
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare densent....\n"
     ]
    }
   ],
   "source": [
    "print(\"prepare densent....\")\n",
    "norm_mean, norm_var = 0.0, 1.0\n",
    "cov_cfg=[(3*i+1) for i in range(12*3+2+1)]\n",
    "class DenseBasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes, dropRate=0):\n",
    "        super(DenseBasicBlock, self).__init__()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(inplanes, outplanes, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "        self.dropRate = dropRate\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        if self.dropRate > 0:\n",
    "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
    "\n",
    "        out = torch.cat((x, out), 1)\n",
    "\n",
    "        return out\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(inplanes, outplanes, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "class DenseNet(nn.Module):\n",
    "\n",
    "    def __init__(self, compress_rate, depth=40, block=DenseBasicBlock,\n",
    "        dropRate=0, num_classes=9, growthRate=12, compressionRate=1):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.compress_rate=compress_rate\n",
    "\n",
    "        assert (depth - 4) % 3 == 0, 'depth should be 3n+4'\n",
    "        n = (depth - 4) // 3 if 'DenseBasicBlock' in str(block) else (depth - 4) // 6\n",
    "\n",
    "        transition = Transition\n",
    "\n",
    "        self.covcfg=cov_cfg\n",
    "\n",
    "        self.growthRate = growthRate\n",
    "        self.dropRate = dropRate\n",
    "\n",
    "        self.inplanes = growthRate * 2\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "\n",
    "        self.dense1 = self._make_denseblock(block, n, compress_rate[1:n+1])\n",
    "        self.trans1 = self._make_transition(transition, compressionRate, compress_rate[n+1])\n",
    "        self.dense2 = self._make_denseblock(block, n, compress_rate[n+2:2*n+2])\n",
    "        self.trans2 = self._make_transition(transition, compressionRate, compress_rate[2*n+2])\n",
    "        self.dense3 = self._make_denseblock(block, n, compress_rate[2*n+3:3*n+3])\n",
    "        self.bn = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "\n",
    "        self.fc = nn.Linear(self.inplanes, num_classes)\n",
    "\n",
    "        # Weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_denseblock(self, block, blocks, compress_rate):\n",
    "        layers = []\n",
    "        for i in range(blocks):\n",
    "            layers.append(block(self.inplanes, outplanes=int(self.growthRate*(1-compress_rate[i])), dropRate=self.dropRate))\n",
    "            self.inplanes += int(self.growthRate*(1-compress_rate[i]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_transition(self, transition, compressionRate, compress_rate):\n",
    "        inplanes = self.inplanes\n",
    "        outplanes = int(math.floor(self.inplanes*(1-compress_rate) // compressionRate))\n",
    "        self.inplanes = outplanes\n",
    "        return transition(inplanes, outplanes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.trans1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.trans2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def densenet_40(compress_rate):\n",
    "    return DenseNet(compress_rate=compress_rate, depth=40, block=DenseBasicBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:40.396708Z",
     "start_time": "2020-11-02T00:58:39.426821Z"
    },
    "code_folding": [
     0,
     2,
     4,
     6,
     10
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if arch == \"resnet_56\":\n",
    "    net = resnet_56(compress_rate=[0.]*100)\n",
    "if arch ==  \"resnet_110\":\n",
    "    net = resnet_110(compress_rate=[0.]*100)\n",
    "if arch == \"vgg\":\n",
    "    net = vgg_16_bn(compress_rate=[0.]*100)\n",
    "if arch == \"densnet\":\n",
    "    net = densenet_40(compress_rate=[0.]*100)\n",
    "if arch == \"mobilenetV2\":\n",
    "    net = mobilenet_v2(compress_rate=[0.]*100,n_class=10)\n",
    "if arch == \"goolenet\":\n",
    "    net = googlenet(compress_rate=[0.]*100)\n",
    "net = net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:40.403714Z",
     "start_time": "2020-11-02T00:58:40.397709Z"
    },
    "code_folding": [
     0,
     25
    ]
   },
   "outputs": [],
   "source": [
    "def inference():\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    limit = 5\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            #use the first 5 batches to estimate the rank.\n",
    "            if batch_idx >= limit:\n",
    "               break\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "#             progress_bar(batch_idx, limit, 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))#'''      \n",
    "def get_feature_hook(self, input, output):\n",
    "    global feature_result\n",
    "    global entropy\n",
    "    global total\n",
    "    a = output.shape[0]\n",
    "    b = output.shape[1]\n",
    "    c = torch.tensor([torch.matrix_rank(output[i,j,:,:]).item() for i in range(a) for j in range(b)])\n",
    "\n",
    "    c = c.view(a, -1).float()\n",
    "    c = c.sum(0)\n",
    "    feature_result = feature_result * total + c\n",
    "    total = total + a\n",
    "    feature_result = feature_result / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:40.430739Z",
     "start_time": "2020-11-02T00:58:40.404715Z"
    },
    "code_folding": [
     15,
     16,
     43,
     59,
     73,
     95,
     110,
     118,
     123,
     138,
     186,
     205
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import time, datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "\n",
    "'''record configurations'''\n",
    "class record_config():\n",
    "    def __init__(self, args):\n",
    "        now = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "        today = datetime.date.today()\n",
    "\n",
    "        self.args = args\n",
    "        self.job_dir = Path(args.job_dir)\n",
    "\n",
    "        def _make_dir(path):\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "        _make_dir(self.job_dir)\n",
    "\n",
    "        config_dir = self.job_dir / 'config.txt'\n",
    "        #if not os.path.exists(config_dir):\n",
    "        if args.resume:\n",
    "            with open(config_dir, 'a') as f:\n",
    "                f.write(now + '\\n\\n')\n",
    "                for arg in vars(args):\n",
    "                    f.write('{}: {}\\n'.format(arg, getattr(args, arg)))\n",
    "                f.write('\\n')\n",
    "        else:\n",
    "            with open(config_dir, 'w') as f:\n",
    "                f.write(now + '\\n\\n')\n",
    "                for arg in vars(args):\n",
    "                    f.write('{}: {}\\n'.format(arg, getattr(args, arg)))\n",
    "                f.write('\\n')\n",
    "def get_logger(file_path):\n",
    "\n",
    "    logger = logging.getLogger('gal')\n",
    "    log_format = '%(asctime)s | %(message)s'\n",
    "    formatter = logging.Formatter(log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "    file_handler = logging.FileHandler(file_path)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    return logger\n",
    "#label smooth\n",
    "class CrossEntropyLabelSmooth(nn.Module):\n",
    "\n",
    "  def __init__(self, num_classes, epsilon):\n",
    "    super(CrossEntropyLabelSmooth, self).__init__()\n",
    "    self.num_classes = num_classes\n",
    "    self.epsilon = epsilon\n",
    "    self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, inputs, targets):\n",
    "    log_probs = self.logsoftmax(inputs)\n",
    "    targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)\n",
    "    targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
    "    loss = (-targets * log_probs).mean(0).sum()\n",
    "    return loss\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print(' '.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "def save_checkpoint(state, is_best, save):\n",
    "    if not os.path.exists(save):\n",
    "        os.makedirs(save)\n",
    "    filename = os.path.join(save, 'checkpoint.pth.tar')\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        best_filename = os.path.join(save, 'model_best.pth.tar')\n",
    "        shutil.copyfile(filename, best_filename)\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "def progress_bar(current, total, msg=None):\n",
    "    _, term_width = os.popen('stty size', 'r').read().split()\n",
    "    term_width = int(term_width)\n",
    "\n",
    "    TOTAL_BAR_LENGTH = 65.\n",
    "    last_time = time.time()\n",
    "    begin_time = last_time\n",
    "\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:40.434742Z",
     "start_time": "2020-11-02T00:58:40.431740Z"
    }
   },
   "outputs": [],
   "source": [
    "arch = \"mobileNetV2\" #[mobileNetV2,resnet_50,vgg] \n",
    "limit = 5\n",
    "#\"./data/model/Hrank_preTrain/cifar-10/resnet_56.pt.pt\" \n",
    "#\"./data/model/Hrank_preTrain/teaLevel/resnet_56_teaLevel.t7\"\n",
    "pretrain_dir = \"./data/model/Hrank_preTrain/cifar-10/mobilev2_pre_91-530.t7\"\n",
    "gpu = '0'\n",
    "save_dir = \"./data/model/rank_conv/cifar-10/\"\n",
    "feature_result = torch.tensor(0.)\n",
    "total = torch.tensor(0.)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:40.519820Z",
     "start_time": "2020-11-02T00:58:40.435744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['net', 'acc'])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MobileNetV2:\n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([100, 1280]) from checkpoint, the shape in current model is torch.Size([10, 1280]).\n\tsize mismatch for classifier.1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e630079d9468>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrain_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'net'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 847\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    848\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MobileNetV2:\n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([100, 1280]) from checkpoint, the shape in current model is torch.Size([10, 1280]).\n\tsize mismatch for classifier.1.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(pretrain_dir)\n",
    "print(checkpoint.keys())\n",
    "net.load_state_dict(checkpoint['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:40.520821Z",
     "start_time": "2020-11-02T00:58:37.360Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if arch == \"mobilenetV2\":\n",
    "    cov_layer = net.features[0]\n",
    "    handler = cov_layer.register_forward_hook(get_feature_hook)\n",
    "    inference()\n",
    "    handler.remove()\n",
    "\n",
    "    # 构建保存文件夹\n",
    "    if not os.path.isdir(save_dir + \"mobileNetV2\"+'_limit%d'%(limit)):\n",
    "        os.mkdir(save_dir + \"mobileNetV2\"+'_limit%d'%(limit))\n",
    "    # 保存\n",
    "    np.save(save_dir+ \"mobileNetV2\"+'_limit%d'%(limit)+ '/rank_conv%d' % (1) + '.npy', feature_result.numpy())\n",
    "    feature_result = torch.tensor(0.)\n",
    "    total = torch.tensor(0.)\n",
    "\n",
    "    cnt=1\n",
    "    for i in range(1,19):\n",
    "        print(\"process on layer {}\".format(i))\n",
    "        if i==1:\n",
    "            block = eval('net.features[%d].conv' % (i))\n",
    "            relu_list=[2,4]\n",
    "        elif i==18:\n",
    "            block = eval('net.features[%d]' % (i))\n",
    "            relu_list=[2]\n",
    "        else:\n",
    "            block = eval('net.features[%d].conv' % (i))\n",
    "            relu_list = [2,5,7]\n",
    "\n",
    "        for j in relu_list:\n",
    "            cov_layer = block[j]\n",
    "            handler = cov_layer.register_forward_hook(get_feature_hook)\n",
    "            inference()\n",
    "            handler.remove()\n",
    "            np.save(save_dir + \"mobileNetV2\" +'_limit%d'%(limit)+ '/rank_conv%d'%(cnt + 1)+'.npy', feature_result.numpy())\n",
    "            cnt+=1\n",
    "            feature_result = torch.tensor(0.)\n",
    "            total = torch.tensor(0.)\n",
    "        print(\"process %dlayer\"%cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:40.521822Z",
     "start_time": "2020-11-02T00:58:37.361Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if arch=='resnet_110':\n",
    "\n",
    "    cov_layer = eval('net.relu')\n",
    "    handler = cov_layer.register_forward_hook(get_feature_hook)\n",
    "    inference()\n",
    "    handler.remove()\n",
    "\n",
    "    if not os.path.isdir(save_dir + arch+'_limit%d'%(limit)):\n",
    "        os.mkdir(save_dir + arch+'_limit%d'%(limit))\n",
    "    np.save(save_dir + arch+'_limit%d'%(limit) + '/rank_conv%d' % (1) + '.npy', feature_result.numpy())\n",
    "    feature_result = torch.tensor(0.)\n",
    "    total = torch.tensor(0.)\n",
    "\n",
    "    cnt = 1\n",
    "    # ResNet110 per block\n",
    "    for i in range(3):\n",
    "        block = eval('net.layer%d' % (i + 1))\n",
    "        for j in range(18):\n",
    "            cov_layer = block[j].relu1\n",
    "            handler = cov_layer.register_forward_hook(get_feature_hook)\n",
    "            inference()\n",
    "            handler.remove()\n",
    "            np.save(save_dir + arch  + '_limit%d' % (limit) + '/rank_conv%d' % (\n",
    "            cnt + 1) + '.npy', feature_result.numpy())\n",
    "            cnt += 1\n",
    "            feature_result = torch.tensor(0.)\n",
    "            total = torch.tensor(0.)\n",
    "\n",
    "            cov_layer = block[j].relu2\n",
    "            handler = cov_layer.register_forward_hook(get_feature_hook)\n",
    "            inference()\n",
    "            handler.remove()\n",
    "            np.save(save_dir + arch  + '_limit%d' % (limit) + '/rank_conv%d' % (\n",
    "                cnt + 1) + '.npy', feature_result.numpy())\n",
    "            cnt += 1\n",
    "            feature_result = torch.tensor(0.)\n",
    "            total = torch.tensor(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:40.522823Z",
     "start_time": "2020-11-02T00:58:37.362Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if arch=='resnet_56':\n",
    "\n",
    "    cov_layer = eval('net.relu')\n",
    "    handler = cov_layer.register_forward_hook(get_feature_hook)\n",
    "    inference()\n",
    "    handler.remove()\n",
    "\n",
    "    if not os.path.isdir(save_dir + arch+'_limit%d'%(limit)):\n",
    "        os.mkdir(save_dir + arch+'_limit%d'%(limit))\n",
    "    np.save(save_dir + arch+'_limit%d'%(limit)+ '/rank_conv%d' % (1) + '.npy', feature_result.numpy())\n",
    "    feature_result = torch.tensor(0.)\n",
    "    total = torch.tensor(0.)\n",
    "\n",
    "    # ResNet56 per block\n",
    "    cnt=1\n",
    "    for i in range(3):\n",
    "        block = eval('net.layer%d' % (i + 1))\n",
    "        for j in range(9):\n",
    "            print(\"process %dlayer\"%cnt)\n",
    "            cov_layer = block[j].relu1\n",
    "            handler = cov_layer.register_forward_hook(get_feature_hook)\n",
    "            inference()\n",
    "            handler.remove()\n",
    "            np.save(save_dir + arch +'_limit%d'%(limit)+ '/rank_conv%d'%(cnt + 1)+'.npy', feature_result.numpy())\n",
    "            cnt+=1\n",
    "            feature_result = torch.tensor(0.)\n",
    "            total = torch.tensor(0.)\n",
    "\n",
    "            cov_layer = block[j].relu2\n",
    "            handler = cov_layer.register_forward_hook(get_feature_hook)\n",
    "            inference()\n",
    "            handler.remove()\n",
    "            np.save(save_dir + arch +'_limit%d'%(limit)+ '/rank_conv%d'%(cnt + 1)+'.npy', feature_result.numpy())\n",
    "            cnt += 1\n",
    "            feature_result = torch.tensor(0.)\n",
    "            total = torch.tensor(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:58:40.522823Z",
     "start_time": "2020-11-02T00:58:37.362Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if arch=='vgg':\n",
    "\n",
    "    relucfg = net.relucfg\n",
    "\n",
    "    for i, cov_id in enumerate(relucfg):\n",
    "        cov_layer = net.features[cov_id]\n",
    "        handler = cov_layer.register_forward_hook(get_feature_hook)\n",
    "        inference()\n",
    "        handler.remove()\n",
    "\n",
    "        if not os.path.isdir(save_dir+arch+'_limit%d'%(limit)):\n",
    "            os.mkdir(save_dir+arch+'_limit%d'%(limit))\n",
    "        np.save(save_dir+arch+'_limit%d'%(limit)+'/rank_conv' + str(i + 1) + '.npy', feature_result.numpy())\n",
    "\n",
    "        feature_result = torch.tensor(0.)\n",
    "        total = torch.tensor(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
