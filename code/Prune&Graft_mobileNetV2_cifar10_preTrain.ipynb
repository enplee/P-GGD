{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T01:00:38.427732Z",
     "start_time": "2020-11-02T01:00:38.410715Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import collections\n",
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import copy\n",
    "from thop import profile\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "import torch.utils\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T01:00:39.228464Z",
     "start_time": "2020-11-02T01:00:39.206444Z"
    },
    "code_folding": [
     4,
     11,
     18,
     23,
     64
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pdb\n",
    "\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "def make_divisible(x, divisible_by=8):\n",
    "    import numpy as np\n",
    "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, compress_rate, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t-ex, c-channel, n-blocknum, s-stride\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 1],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "        self.compress_rate=compress_rate[:]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        # input_channel = make_divisible(input_channel * width_mult)  # first channel is always 32!\n",
    "        self.last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        cnt=1\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = make_divisible(c * width_mult) if t > 1 else c\n",
    "            output_channel = int((1-self.compress_rate[cnt])*output_channel)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "            cnt+=1\n",
    "\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        #self.classifier = nn.Linear(self.last_channel, n_class)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def mobilenet_v2(compress_rate,n_class=1000):\n",
    "    model = MobileNetV2(compress_rate=compress_rate,n_class=n_class,width_mult=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T01:01:20.493252Z",
     "start_time": "2020-11-02T01:01:20.469230Z"
    },
    "code_folding": [
     1,
     29,
     34,
     39,
     48,
     97
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare resNet_50 model...\n"
     ]
    }
   ],
   "source": [
    "print(\"prepare resNet_50 model...\")\n",
    "def adapt_channel(compress_rate, num_layers):\n",
    "\n",
    "    if num_layers==56:\n",
    "        stage_repeat = [9, 9, 9]\n",
    "        stage_out_channel = [16] + [16] * 9 + [32] * 9 + [64] * 9\n",
    "    elif num_layers==110:\n",
    "        stage_repeat = [18, 18, 18]\n",
    "        stage_out_channel = [16] + [16] * 18 + [32] * 18 + [64] * 18\n",
    "\n",
    "    stage_oup_cprate = []\n",
    "    stage_oup_cprate += [compress_rate[0]]\n",
    "    for i in range(len(stage_repeat)-1):\n",
    "        stage_oup_cprate += [compress_rate[i+1]] * stage_repeat[i]\n",
    "    stage_oup_cprate +=[0.] * stage_repeat[-1]\n",
    "    mid_cprate = compress_rate[len(stage_repeat):]\n",
    "\n",
    "    overall_channel = []\n",
    "    mid_channel = []\n",
    "    for i in range(len(stage_out_channel)):\n",
    "        if i == 0 :\n",
    "            overall_channel += [int(stage_out_channel[i] * (1-stage_oup_cprate[i]))]\n",
    "        else:\n",
    "            overall_channel += [int(stage_out_channel[i] * (1-stage_oup_cprate[i]))]\n",
    "            mid_channel += [int(stage_out_channel[i] * (1-mid_cprate[i-1]))]\n",
    "\n",
    "    return overall_channel, mid_channel\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, midplanes, inplanes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.planes = planes\n",
    "        self.conv1 = conv3x3(inplanes, midplanes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(midplanes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = conv3x3(midplanes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.stride = stride\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inplanes != planes:\n",
    "            if stride!=1:\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(x[:, :, ::2, ::2],\n",
    "                                    (0, 0, 0, 0, (planes-inplanes)//2, planes-inplanes-(planes-inplanes)//2), \"constant\", 0))\n",
    "            else:\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(x[:, :, :, :],\n",
    "                                    (0, 0, 0, 0, (planes-inplanes)//2, planes-inplanes-(planes-inplanes)//2), \"constant\", 0))\n",
    "            #self.shortcut = LambdaLayer(\n",
    "            #    lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4),\"constant\", 0))\n",
    "\n",
    "            '''self.shortcut = nn.Sequential(\n",
    "                conv1x1(inplanes, planes, stride=stride),\n",
    "                #nn.BatchNorm2d(planes),\n",
    "            )#'''\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        #print(self.stride, self.inplanes, self.planes, out.size(), self.shortcut(x).size())\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_layers, compress_rate, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert (num_layers - 2) % 6 == 0, 'depth should be 6n+2'\n",
    "        n = (num_layers - 2) // 6\n",
    "\n",
    "        self.num_layer = num_layers\n",
    "        self.overall_channel, self.mid_channel = adapt_channel(compress_rate, num_layers)\n",
    "\n",
    "        self.layer_num = 0\n",
    "        self.conv1 = nn.Conv2d(3, self.overall_channel[self.layer_num], kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.overall_channel[self.layer_num])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_num += 1\n",
    "\n",
    "        #self.layers = nn.ModuleList()\n",
    "        self.layer1 = self._make_layer(block, blocks_num=n, stride=1)\n",
    "        self.layer2 = self._make_layer(block, blocks_num=n, stride=2)\n",
    "        self.layer3 = self._make_layer(block, blocks_num=n, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        if self.num_layer == 56:\n",
    "            self.fc = nn.Linear(64 * BasicBlock.expansion, num_classes)\n",
    "        else:\n",
    "            self.linear = nn.Linear(64 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, blocks_num, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.mid_channel[self.layer_num - 1], self.overall_channel[self.layer_num - 1],\n",
    "                                 self.overall_channel[self.layer_num], stride))\n",
    "        self.layer_num += 1\n",
    "\n",
    "        for i in range(1, blocks_num):\n",
    "            layers.append(block(self.mid_channel[self.layer_num - 1], self.overall_channel[self.layer_num - 1],\n",
    "                                     self.overall_channel[self.layer_num]))\n",
    "            self.layer_num += 1\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        for i, block in enumerate(self.layer1):\n",
    "            x = block(x)\n",
    "        for i, block in enumerate(self.layer2):\n",
    "            x = block(x)\n",
    "        for i, block in enumerate(self.layer3):\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.num_layer == 56:\n",
    "            x = self.fc(x)\n",
    "        else:\n",
    "            x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet_56(compress_rate):\n",
    "    return ResNet(BasicBlock, 56, compress_rate=compress_rate)\n",
    "\n",
    "def resnet_110(compress_rate):\n",
    "    return ResNet(BasicBlock, 110, compress_rate=compress_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T01:01:21.091799Z",
     "start_time": "2020-11-02T01:01:21.088797Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "cudnn.benchmark = True\n",
    "cudnn.enabled=True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T01:01:21.508180Z",
     "start_time": "2020-11-02T01:01:21.505178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "超参数\n"
     ]
    }
   ],
   "source": [
    "print(\"超参数\")\n",
    "CLASSES = 10\n",
    "epochs = 400\n",
    "batch_size=  256\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "# 0.006\n",
    "weight_decay = 0.005\n",
    "lr_decay_step = '150,225'\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "save_dir = \"./data/model/Hrank_preTrain/cifar-10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T01:01:22.449041Z",
     "start_time": "2020-11-02T01:01:22.336938Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = mobilenet_v2(compress_rate=[0.]*100,n_class=10)\n",
    "net = net.to(device)\n",
    "print(net)\n",
    "class CrossEntropyLabelSmooth(nn.Module):\n",
    "\n",
    "  def __init__(self, num_classes, epsilon):\n",
    "    super(CrossEntropyLabelSmooth, self).__init__()\n",
    "    self.num_classes = num_classes\n",
    "    self.epsilon = epsilon\n",
    "    self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, inputs, targets):\n",
    "    log_probs = self.logsoftmax(inputs)\n",
    "    targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)\n",
    "    targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
    "    loss = (-targets * log_probs).mean(0).sum()\n",
    "    return loss   \n",
    "\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.cuda()\n",
    "criterion_smooth = CrossEntropyLabelSmooth(CLASSES, 0.1)\n",
    "criterion_smooth = criterion_smooth.cuda()\n",
    "lr_decay_step = list(map(int, lr_decay_step.split(',')))\n",
    "optimizer = torch.optim.SGD(net.parameters(), learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_decay_step, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T01:01:25.679500Z",
     "start_time": "2020-11-02T01:01:25.677499Z"
    }
   },
   "outputs": [],
   "source": [
    "# input_image_size=32\n",
    "# input_image = torch.randn(1, 3, input_image_size, input_image_size).cuda()\n",
    "# flops, params = profile(net, inputs=(input_image,))\n",
    "# print('Params: %.2f' % (params))\n",
    "# print('Flops: %.2f' % (flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T01:01:28.090706Z",
     "start_time": "2020-11-02T01:01:26.567313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load training data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "print('load training data')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data/cifar-10-batches-py/', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data/cifar-10-batches-py/', train=False, download=True, transform=transform_test)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T01:01:30.031986Z",
     "start_time": "2020-11-02T01:01:30.024980Z"
    },
    "code_folding": [
     0,
     22
    ]
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T01:01:31.583405Z",
     "start_time": "2020-11-02T01:01:31.573396Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch,i,net,optimizer,scheduler):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    losses =AverageMeter('Loss', ':.4e')\n",
    "    top1 =AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 =AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "    num_iter = len(trainloader)\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "        \n",
    "        n = inputs.size(0)\n",
    "        losses.update(loss.item(), n)  # accumulated loss\n",
    "        top1.update(prec1.item(), n)\n",
    "        top5.update(prec5.item(), n)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0 and batch_idx != 0:\n",
    "            print(\n",
    "                'Epoch[{0}]({1}/{2}): '\n",
    "                'Loss {loss.avg:.4f} '\n",
    "                'Prec@1(1,5) {top1.avg:.2f}, {top5.avg:.2f}'.format(\n",
    "                    epoch, batch_idx, num_iter, loss=losses,\n",
    "                    top1=top1, top5=top5))\n",
    "    scheduler.step()\n",
    "    \n",
    "def test(epoch,net,i):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    losses =AverageMeter('Loss', ':.4e')\n",
    "    top1 =AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 =AverageMeter('Acc@5', ':6.2f')\n",
    "    num_iter = len(testloader)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            pred1, pred5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            n = inputs.size(0)\n",
    "            losses.update(loss.item(), n)\n",
    "            top1.update(pred1[0], n)\n",
    "            top5.update(pred5[0], n)\n",
    "    valid_top1_acc = top1.avg\n",
    "    valid_top5_acc = top5.avg\n",
    "    \n",
    "    if valid_top1_acc > best_acc:\n",
    "        best_acc = valid_top1_acc\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': best_acc\n",
    "        }\n",
    "        torch.save(state, save_dir+'best_%d.t7' % (epoch))\n",
    "    print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "                    .format(top1=top1, top5=top5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T03:03:45.061769Z",
     "start_time": "2020-11-02T01:01:32.743467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0](100/196): Loss 2.1693 Prec@1(1,5) 25.47, 76.69\n",
      " * Acc@1 43.870 Acc@5 92.100\n",
      "Epoch[1](100/196): Loss 1.5062 Prec@1(1,5) 44.82, 91.33\n",
      " * Acc@1 51.110 Acc@5 93.590\n",
      "Epoch[2](100/196): Loss 1.3318 Prec@1(1,5) 51.56, 93.88\n",
      " * Acc@1 57.140 Acc@5 95.490\n",
      "Epoch[3](100/196): Loss 1.1759 Prec@1(1,5) 58.04, 95.29\n",
      " * Acc@1 60.040 Acc@5 95.880\n",
      "Epoch[4](100/196): Loss 1.0780 Prec@1(1,5) 61.47, 96.16\n",
      " * Acc@1 64.000 Acc@5 96.470\n",
      "Epoch[5](100/196): Loss 0.9980 Prec@1(1,5) 64.99, 96.79\n",
      " * Acc@1 64.780 Acc@5 96.570\n",
      "Epoch[6](100/196): Loss 0.9254 Prec@1(1,5) 67.48, 97.25\n",
      " * Acc@1 67.370 Acc@5 97.250\n",
      "Epoch[7](100/196): Loss 0.8570 Prec@1(1,5) 69.87, 97.77\n",
      " * Acc@1 71.720 Acc@5 97.650\n",
      "Epoch[8](100/196): Loss 0.8003 Prec@1(1,5) 72.03, 97.94\n",
      " * Acc@1 72.580 Acc@5 97.960\n",
      "Epoch[9](100/196): Loss 0.7440 Prec@1(1,5) 73.95, 98.38\n",
      " * Acc@1 73.880 Acc@5 98.370\n",
      "Epoch[10](100/196): Loss 0.7007 Prec@1(1,5) 75.58, 98.41\n",
      " * Acc@1 75.000 Acc@5 98.200\n",
      "Epoch[11](100/196): Loss 0.6534 Prec@1(1,5) 77.16, 98.66\n",
      " * Acc@1 74.390 Acc@5 98.200\n",
      "Epoch[12](100/196): Loss 0.6295 Prec@1(1,5) 78.31, 98.89\n",
      " * Acc@1 76.440 Acc@5 98.680\n",
      "Epoch[13](100/196): Loss 0.5978 Prec@1(1,5) 79.02, 98.96\n",
      " * Acc@1 77.140 Acc@5 98.810\n",
      "Epoch[14](100/196): Loss 0.5729 Prec@1(1,5) 79.84, 98.96\n",
      " * Acc@1 79.400 Acc@5 98.950\n",
      "Epoch[15](100/196): Loss 0.5529 Prec@1(1,5) 80.54, 99.03\n",
      " * Acc@1 78.970 Acc@5 98.820\n",
      "Epoch[16](100/196): Loss 0.5288 Prec@1(1,5) 81.48, 99.11\n",
      " * Acc@1 78.590 Acc@5 98.830\n",
      "Epoch[17](100/196): Loss 0.5226 Prec@1(1,5) 82.02, 99.17\n",
      " * Acc@1 78.130 Acc@5 98.800\n",
      "Epoch[18](100/196): Loss 0.5106 Prec@1(1,5) 82.52, 99.15\n",
      " * Acc@1 80.900 Acc@5 99.090\n",
      "Epoch[19](100/196): Loss 0.4892 Prec@1(1,5) 82.79, 99.25\n",
      " * Acc@1 80.270 Acc@5 99.020\n",
      "Epoch[20](100/196): Loss 0.4820 Prec@1(1,5) 83.39, 99.29\n",
      " * Acc@1 81.410 Acc@5 99.110\n",
      "Epoch[21](100/196): Loss 0.4641 Prec@1(1,5) 83.64, 99.41\n",
      " * Acc@1 82.760 Acc@5 99.240\n",
      "Epoch[22](100/196): Loss 0.4527 Prec@1(1,5) 84.23, 99.42\n",
      " * Acc@1 80.800 Acc@5 98.930\n",
      "Epoch[23](100/196): Loss 0.4405 Prec@1(1,5) 84.62, 99.41\n",
      " * Acc@1 81.160 Acc@5 99.120\n",
      "Epoch[24](100/196): Loss 0.4295 Prec@1(1,5) 84.99, 99.44\n",
      " * Acc@1 80.970 Acc@5 98.970\n",
      "Epoch[25](100/196): Loss 0.4210 Prec@1(1,5) 85.29, 99.49\n",
      " * Acc@1 81.020 Acc@5 99.080\n",
      "Epoch[26](100/196): Loss 0.4188 Prec@1(1,5) 85.43, 99.51\n",
      " * Acc@1 82.130 Acc@5 99.090\n",
      "Epoch[27](100/196): Loss 0.4106 Prec@1(1,5) 85.74, 99.50\n",
      " * Acc@1 83.990 Acc@5 99.340\n",
      "Epoch[28](100/196): Loss 0.3929 Prec@1(1,5) 86.35, 99.55\n",
      " * Acc@1 82.670 Acc@5 99.200\n",
      "Epoch[29](100/196): Loss 0.3907 Prec@1(1,5) 86.24, 99.58\n",
      " * Acc@1 83.150 Acc@5 99.240\n",
      "Epoch[30](100/196): Loss 0.3755 Prec@1(1,5) 86.85, 99.52\n",
      " * Acc@1 83.910 Acc@5 99.200\n",
      "Epoch[31](100/196): Loss 0.3734 Prec@1(1,5) 87.05, 99.65\n",
      " * Acc@1 82.860 Acc@5 99.320\n",
      "Epoch[32](100/196): Loss 0.3678 Prec@1(1,5) 87.11, 99.58\n",
      " * Acc@1 83.280 Acc@5 99.200\n",
      "Epoch[33](100/196): Loss 0.3647 Prec@1(1,5) 86.99, 99.65\n",
      " * Acc@1 85.010 Acc@5 99.340\n",
      "Epoch[34](100/196): Loss 0.3505 Prec@1(1,5) 87.57, 99.60\n",
      " * Acc@1 83.510 Acc@5 99.300\n",
      "Epoch[35](100/196): Loss 0.3506 Prec@1(1,5) 87.80, 99.70\n",
      " * Acc@1 83.370 Acc@5 99.310\n",
      "Epoch[36](100/196): Loss 0.3391 Prec@1(1,5) 88.15, 99.71\n",
      " * Acc@1 83.520 Acc@5 99.340\n",
      "Epoch[37](100/196): Loss 0.3389 Prec@1(1,5) 88.15, 99.71\n",
      " * Acc@1 84.810 Acc@5 99.300\n",
      "Epoch[38](100/196): Loss 0.3368 Prec@1(1,5) 88.05, 99.73\n",
      " * Acc@1 84.010 Acc@5 99.360\n",
      "Epoch[39](100/196): Loss 0.3190 Prec@1(1,5) 88.83, 99.70\n",
      " * Acc@1 84.940 Acc@5 99.180\n",
      "Epoch[40](100/196): Loss 0.3226 Prec@1(1,5) 88.89, 99.65\n",
      " * Acc@1 85.540 Acc@5 99.370\n",
      "Epoch[41](100/196): Loss 0.3187 Prec@1(1,5) 88.88, 99.75\n",
      " * Acc@1 85.360 Acc@5 99.420\n",
      "Epoch[42](100/196): Loss 0.3148 Prec@1(1,5) 88.80, 99.72\n",
      " * Acc@1 85.180 Acc@5 99.390\n",
      "Epoch[43](100/196): Loss 0.3155 Prec@1(1,5) 88.85, 99.77\n",
      " * Acc@1 85.920 Acc@5 99.350\n",
      "Epoch[44](100/196): Loss 0.3060 Prec@1(1,5) 89.31, 99.73\n",
      " * Acc@1 84.610 Acc@5 99.400\n",
      "Epoch[45](100/196): Loss 0.3013 Prec@1(1,5) 89.60, 99.74\n",
      " * Acc@1 85.350 Acc@5 99.280\n",
      "Epoch[46](100/196): Loss 0.3075 Prec@1(1,5) 89.22, 99.76\n",
      " * Acc@1 85.410 Acc@5 99.350\n",
      "Epoch[47](100/196): Loss 0.2988 Prec@1(1,5) 89.64, 99.76\n",
      " * Acc@1 85.750 Acc@5 99.360\n",
      "Epoch[48](100/196): Loss 0.3021 Prec@1(1,5) 89.36, 99.73\n",
      " * Acc@1 84.820 Acc@5 99.300\n",
      "Epoch[49](100/196): Loss 0.2972 Prec@1(1,5) 89.46, 99.79\n",
      " * Acc@1 84.880 Acc@5 99.320\n",
      "Epoch[50](100/196): Loss 0.2873 Prec@1(1,5) 89.90, 99.83\n",
      " * Acc@1 84.050 Acc@5 99.300\n",
      "Epoch[51](100/196): Loss 0.2921 Prec@1(1,5) 89.77, 99.74\n",
      " * Acc@1 86.750 Acc@5 99.450\n",
      "Epoch[52](100/196): Loss 0.2812 Prec@1(1,5) 89.99, 99.82\n",
      " * Acc@1 86.630 Acc@5 99.440\n",
      "Epoch[53](100/196): Loss 0.2836 Prec@1(1,5) 89.96, 99.82\n",
      " * Acc@1 85.530 Acc@5 99.370\n",
      "Epoch[54](100/196): Loss 0.2814 Prec@1(1,5) 90.13, 99.82\n",
      " * Acc@1 86.030 Acc@5 99.390\n",
      "Epoch[55](100/196): Loss 0.2829 Prec@1(1,5) 90.00, 99.83\n",
      " * Acc@1 86.120 Acc@5 99.540\n",
      "Epoch[56](100/196): Loss 0.2741 Prec@1(1,5) 90.49, 99.85\n",
      " * Acc@1 84.610 Acc@5 99.220\n",
      "Epoch[57](100/196): Loss 0.2759 Prec@1(1,5) 90.31, 99.85\n",
      " * Acc@1 86.260 Acc@5 99.470\n",
      "Epoch[58](100/196): Loss 0.2711 Prec@1(1,5) 90.22, 99.83\n",
      " * Acc@1 85.690 Acc@5 99.450\n",
      "Epoch[59](100/196): Loss 0.2574 Prec@1(1,5) 90.99, 99.84\n",
      " * Acc@1 86.370 Acc@5 99.420\n",
      "Epoch[60](100/196): Loss 0.2702 Prec@1(1,5) 90.54, 99.84\n",
      " * Acc@1 86.970 Acc@5 99.470\n",
      "Epoch[61](100/196): Loss 0.2666 Prec@1(1,5) 90.60, 99.85\n",
      " * Acc@1 86.160 Acc@5 99.470\n",
      "Epoch[62](100/196): Loss 0.2640 Prec@1(1,5) 90.58, 99.85\n",
      " * Acc@1 85.280 Acc@5 99.520\n",
      "Epoch[63](100/196): Loss 0.2667 Prec@1(1,5) 90.60, 99.80\n",
      " * Acc@1 85.420 Acc@5 99.340\n",
      "Epoch[64](100/196): Loss 0.2617 Prec@1(1,5) 90.68, 99.86\n",
      " * Acc@1 85.670 Acc@5 99.390\n",
      "Epoch[65](100/196): Loss 0.2618 Prec@1(1,5) 90.89, 99.85\n",
      " * Acc@1 86.110 Acc@5 99.430\n",
      "Epoch[66](100/196): Loss 0.2536 Prec@1(1,5) 91.14, 99.83\n",
      " * Acc@1 86.750 Acc@5 99.340\n",
      "Epoch[67](100/196): Loss 0.2596 Prec@1(1,5) 90.90, 99.83\n",
      " * Acc@1 85.720 Acc@5 99.370\n",
      "Epoch[68](100/196): Loss 0.2540 Prec@1(1,5) 91.06, 99.81\n",
      " * Acc@1 87.030 Acc@5 99.460\n",
      "Epoch[69](100/196): Loss 0.2516 Prec@1(1,5) 91.12, 99.87\n",
      " * Acc@1 86.210 Acc@5 99.590\n",
      "Epoch[70](100/196): Loss 0.2532 Prec@1(1,5) 91.09, 99.86\n",
      " * Acc@1 86.160 Acc@5 99.450\n",
      "Epoch[71](100/196): Loss 0.2549 Prec@1(1,5) 91.00, 99.81\n",
      " * Acc@1 86.030 Acc@5 99.390\n",
      "Epoch[72](100/196): Loss 0.2489 Prec@1(1,5) 91.25, 99.84\n",
      " * Acc@1 84.900 Acc@5 99.320\n",
      "Epoch[73](100/196): Loss 0.2459 Prec@1(1,5) 91.27, 99.90\n",
      " * Acc@1 86.010 Acc@5 99.390\n",
      "Epoch[74](100/196): Loss 0.2439 Prec@1(1,5) 91.41, 99.86\n",
      " * Acc@1 86.930 Acc@5 99.480\n",
      "Epoch[75](100/196): Loss 0.2423 Prec@1(1,5) 91.50, 99.87\n",
      " * Acc@1 85.710 Acc@5 99.450\n",
      "Epoch[76](100/196): Loss 0.2451 Prec@1(1,5) 91.49, 99.88\n",
      " * Acc@1 86.430 Acc@5 99.430\n",
      "Epoch[77](100/196): Loss 0.2482 Prec@1(1,5) 91.43, 99.87\n",
      " * Acc@1 85.390 Acc@5 99.320\n",
      "Epoch[78](100/196): Loss 0.2499 Prec@1(1,5) 91.15, 99.83\n",
      " * Acc@1 86.540 Acc@5 99.500\n",
      "Epoch[79](100/196): Loss 0.2420 Prec@1(1,5) 91.58, 99.86\n",
      " * Acc@1 86.170 Acc@5 99.460\n",
      "Epoch[80](100/196): Loss 0.2437 Prec@1(1,5) 91.41, 99.86\n",
      " * Acc@1 87.700 Acc@5 99.530\n",
      "Epoch[81](100/196): Loss 0.2352 Prec@1(1,5) 91.92, 99.90\n",
      " * Acc@1 85.780 Acc@5 99.390\n",
      "Epoch[82](100/196): Loss 0.2350 Prec@1(1,5) 91.65, 99.88\n",
      " * Acc@1 87.190 Acc@5 99.530\n",
      "Epoch[83](100/196): Loss 0.2383 Prec@1(1,5) 91.58, 99.86\n",
      " * Acc@1 86.440 Acc@5 99.560\n",
      "Epoch[84](100/196): Loss 0.2391 Prec@1(1,5) 91.71, 99.90\n",
      " * Acc@1 87.300 Acc@5 99.550\n",
      "Epoch[85](100/196): Loss 0.2331 Prec@1(1,5) 91.85, 99.89\n",
      " * Acc@1 87.550 Acc@5 99.470\n",
      "Epoch[86](100/196): Loss 0.2310 Prec@1(1,5) 91.92, 99.89\n",
      " * Acc@1 86.640 Acc@5 99.470\n",
      "Epoch[87](100/196): Loss 0.2303 Prec@1(1,5) 91.82, 99.91\n",
      " * Acc@1 87.750 Acc@5 99.450\n",
      "Epoch[88](100/196): Loss 0.2331 Prec@1(1,5) 91.78, 99.90\n",
      " * Acc@1 86.580 Acc@5 99.510\n",
      "Epoch[89](100/196): Loss 0.2370 Prec@1(1,5) 91.77, 99.85\n",
      " * Acc@1 87.340 Acc@5 99.480\n",
      "Epoch[90](100/196): Loss 0.2364 Prec@1(1,5) 91.73, 99.86\n",
      " * Acc@1 86.710 Acc@5 99.540\n",
      "Epoch[91](100/196): Loss 0.2260 Prec@1(1,5) 92.10, 99.88\n",
      " * Acc@1 87.070 Acc@5 99.470\n",
      "Epoch[92](100/196): Loss 0.2274 Prec@1(1,5) 92.01, 99.88\n",
      " * Acc@1 86.620 Acc@5 99.510\n",
      "Epoch[93](100/196): Loss 0.2200 Prec@1(1,5) 92.40, 99.90\n",
      " * Acc@1 85.880 Acc@5 99.410\n",
      "Epoch[94](100/196): Loss 0.2380 Prec@1(1,5) 91.48, 99.87\n",
      " * Acc@1 87.570 Acc@5 99.420\n",
      "Epoch[95](100/196): Loss 0.2283 Prec@1(1,5) 92.11, 99.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 85.790 Acc@5 99.370\n",
      "Epoch[96](100/196): Loss 0.2310 Prec@1(1,5) 91.85, 99.89\n",
      " * Acc@1 86.260 Acc@5 99.320\n",
      "Epoch[97](100/196): Loss 0.2211 Prec@1(1,5) 92.33, 99.89\n",
      " * Acc@1 86.470 Acc@5 99.480\n",
      "Epoch[98](100/196): Loss 0.2281 Prec@1(1,5) 91.99, 99.91\n",
      " * Acc@1 87.940 Acc@5 99.470\n",
      "Epoch[99](100/196): Loss 0.2330 Prec@1(1,5) 91.87, 99.90\n",
      " * Acc@1 87.940 Acc@5 99.510\n",
      "Epoch[100](100/196): Loss 0.2329 Prec@1(1,5) 91.71, 99.90\n",
      " * Acc@1 86.590 Acc@5 99.530\n",
      "Epoch[101](100/196): Loss 0.2166 Prec@1(1,5) 92.30, 99.91\n",
      " * Acc@1 87.330 Acc@5 99.470\n",
      "Epoch[102](100/196): Loss 0.2155 Prec@1(1,5) 92.44, 99.89\n",
      " * Acc@1 87.360 Acc@5 99.390\n",
      "Epoch[103](100/196): Loss 0.2255 Prec@1(1,5) 92.07, 99.87\n",
      " * Acc@1 87.370 Acc@5 99.450\n",
      "Epoch[104](100/196): Loss 0.2183 Prec@1(1,5) 92.35, 99.87\n",
      " * Acc@1 86.030 Acc@5 99.410\n",
      "Epoch[105](100/196): Loss 0.2208 Prec@1(1,5) 92.23, 99.90\n",
      " * Acc@1 87.580 Acc@5 99.570\n",
      "Epoch[106](100/196): Loss 0.2174 Prec@1(1,5) 92.40, 99.91\n",
      " * Acc@1 86.540 Acc@5 99.480\n",
      "Epoch[107](100/196): Loss 0.2213 Prec@1(1,5) 92.31, 99.88\n",
      " * Acc@1 86.900 Acc@5 99.470\n",
      "Epoch[108](100/196): Loss 0.2195 Prec@1(1,5) 92.30, 99.90\n",
      " * Acc@1 88.090 Acc@5 99.470\n",
      "Epoch[109](100/196): Loss 0.2107 Prec@1(1,5) 92.55, 99.88\n",
      " * Acc@1 86.520 Acc@5 99.440\n",
      "Epoch[110](100/196): Loss 0.2135 Prec@1(1,5) 92.59, 99.88\n",
      " * Acc@1 87.780 Acc@5 99.490\n",
      "Epoch[111](100/196): Loss 0.2121 Prec@1(1,5) 92.47, 99.92\n",
      " * Acc@1 85.330 Acc@5 99.340\n",
      "Epoch[112](100/196): Loss 0.2180 Prec@1(1,5) 92.23, 99.90\n",
      " * Acc@1 86.500 Acc@5 99.430\n",
      "Epoch[113](100/196): Loss 0.2132 Prec@1(1,5) 92.39, 99.89\n",
      " * Acc@1 87.130 Acc@5 99.460\n",
      "Epoch[114](100/196): Loss 0.2140 Prec@1(1,5) 92.51, 99.91\n",
      " * Acc@1 86.440 Acc@5 99.510\n",
      "Epoch[115](100/196): Loss 0.2112 Prec@1(1,5) 92.54, 99.86\n",
      " * Acc@1 86.700 Acc@5 99.620\n",
      "Epoch[116](100/196): Loss 0.2211 Prec@1(1,5) 92.39, 99.86\n",
      " * Acc@1 86.720 Acc@5 99.570\n",
      "Epoch[117](100/196): Loss 0.2093 Prec@1(1,5) 92.74, 99.87\n",
      " * Acc@1 87.580 Acc@5 99.410\n",
      "Epoch[118](100/196): Loss 0.2118 Prec@1(1,5) 92.41, 99.88\n",
      " * Acc@1 86.140 Acc@5 99.550\n",
      "Epoch[119](100/196): Loss 0.2052 Prec@1(1,5) 92.69, 99.88\n",
      " * Acc@1 86.170 Acc@5 99.410\n",
      "Epoch[120](100/196): Loss 0.2113 Prec@1(1,5) 92.42, 99.91\n",
      " * Acc@1 87.040 Acc@5 99.500\n",
      "Epoch[121](100/196): Loss 0.2138 Prec@1(1,5) 92.55, 99.91\n",
      " * Acc@1 86.290 Acc@5 99.510\n",
      "Epoch[122](100/196): Loss 0.2073 Prec@1(1,5) 92.69, 99.92\n",
      " * Acc@1 87.490 Acc@5 99.460\n",
      "Epoch[123](100/196): Loss 0.2173 Prec@1(1,5) 92.28, 99.86\n",
      " * Acc@1 87.050 Acc@5 99.490\n",
      "Epoch[124](100/196): Loss 0.2059 Prec@1(1,5) 92.68, 99.92\n",
      " * Acc@1 86.320 Acc@5 99.350\n",
      "Epoch[125](100/196): Loss 0.2087 Prec@1(1,5) 92.68, 99.93\n",
      " * Acc@1 87.680 Acc@5 99.600\n",
      "Epoch[126](100/196): Loss 0.2142 Prec@1(1,5) 92.42, 99.90\n",
      " * Acc@1 87.150 Acc@5 99.500\n",
      "Epoch[127](100/196): Loss 0.2084 Prec@1(1,5) 92.73, 99.92\n",
      " * Acc@1 87.360 Acc@5 99.510\n",
      "Epoch[128](100/196): Loss 0.2132 Prec@1(1,5) 92.58, 99.90\n",
      " * Acc@1 87.710 Acc@5 99.510\n",
      "Epoch[129](100/196): Loss 0.2015 Prec@1(1,5) 92.88, 99.88\n",
      " * Acc@1 87.870 Acc@5 99.430\n",
      "Epoch[130](100/196): Loss 0.1987 Prec@1(1,5) 92.92, 99.92\n",
      " * Acc@1 87.800 Acc@5 99.610\n",
      "Epoch[131](100/196): Loss 0.2020 Prec@1(1,5) 92.91, 99.92\n",
      " * Acc@1 86.650 Acc@5 99.510\n",
      "Epoch[132](100/196): Loss 0.2061 Prec@1(1,5) 92.75, 99.91\n",
      " * Acc@1 86.480 Acc@5 99.510\n",
      "Epoch[133](100/196): Loss 0.2071 Prec@1(1,5) 92.69, 99.91\n",
      " * Acc@1 86.750 Acc@5 99.510\n",
      "Epoch[134](100/196): Loss 0.2064 Prec@1(1,5) 92.78, 99.95\n",
      " * Acc@1 85.340 Acc@5 99.390\n",
      "Epoch[135](100/196): Loss 0.2091 Prec@1(1,5) 92.50, 99.94\n",
      " * Acc@1 87.210 Acc@5 99.410\n",
      "Epoch[136](100/196): Loss 0.2027 Prec@1(1,5) 92.92, 99.90\n",
      " * Acc@1 86.650 Acc@5 99.460\n",
      "Epoch[137](100/196): Loss 0.2059 Prec@1(1,5) 92.73, 99.93\n",
      " * Acc@1 86.910 Acc@5 99.500\n",
      "Epoch[138](100/196): Loss 0.2043 Prec@1(1,5) 92.89, 99.91\n",
      " * Acc@1 87.700 Acc@5 99.480\n",
      "Epoch[139](100/196): Loss 0.2127 Prec@1(1,5) 92.46, 99.88\n",
      " * Acc@1 86.140 Acc@5 99.470\n",
      "Epoch[140](100/196): Loss 0.1988 Prec@1(1,5) 93.00, 99.87\n",
      " * Acc@1 87.350 Acc@5 99.510\n",
      "Epoch[141](100/196): Loss 0.1936 Prec@1(1,5) 93.15, 99.92\n",
      " * Acc@1 87.150 Acc@5 99.450\n",
      "Epoch[142](100/196): Loss 0.2096 Prec@1(1,5) 92.79, 99.91\n",
      " * Acc@1 88.210 Acc@5 99.530\n",
      "Epoch[143](100/196): Loss 0.2036 Prec@1(1,5) 92.76, 99.93\n",
      " * Acc@1 87.220 Acc@5 99.480\n",
      "Epoch[144](100/196): Loss 0.2054 Prec@1(1,5) 92.74, 99.93\n",
      " * Acc@1 86.360 Acc@5 99.420\n",
      "Epoch[145](100/196): Loss 0.1965 Prec@1(1,5) 93.02, 99.92\n",
      " * Acc@1 86.240 Acc@5 99.430\n",
      "Epoch[146](100/196): Loss 0.2043 Prec@1(1,5) 93.09, 99.92\n",
      " * Acc@1 88.250 Acc@5 99.540\n",
      "Epoch[147](100/196): Loss 0.1924 Prec@1(1,5) 93.15, 99.95\n",
      " * Acc@1 87.340 Acc@5 99.420\n",
      "Epoch[148](100/196): Loss 0.2011 Prec@1(1,5) 92.81, 99.91\n",
      " * Acc@1 87.910 Acc@5 99.520\n",
      "Epoch[149](100/196): Loss 0.2069 Prec@1(1,5) 92.62, 99.88\n",
      " * Acc@1 87.860 Acc@5 99.580\n",
      "Epoch[150](100/196): Loss 0.1327 Prec@1(1,5) 95.57, 99.98\n",
      " * Acc@1 90.640 Acc@5 99.740\n",
      "Epoch[151](100/196): Loss 0.0952 Prec@1(1,5) 96.79, 99.99\n",
      " * Acc@1 90.930 Acc@5 99.740\n",
      "Epoch[152](100/196): Loss 0.0768 Prec@1(1,5) 97.44, 99.97\n",
      " * Acc@1 91.310 Acc@5 99.730\n",
      "Epoch[153](100/196): Loss 0.0697 Prec@1(1,5) 97.65, 99.98\n",
      " * Acc@1 91.450 Acc@5 99.710\n",
      "Epoch[154](100/196): Loss 0.0607 Prec@1(1,5) 97.92, 99.99\n",
      " * Acc@1 91.480 Acc@5 99.690\n",
      "Epoch[155](100/196): Loss 0.0572 Prec@1(1,5) 98.07, 99.99\n",
      " * Acc@1 91.340 Acc@5 99.700\n",
      "Epoch[156](100/196): Loss 0.0544 Prec@1(1,5) 98.15, 99.99\n",
      " * Acc@1 91.400 Acc@5 99.700\n",
      "Epoch[157](100/196): Loss 0.0490 Prec@1(1,5) 98.41, 99.99\n",
      " * Acc@1 91.510 Acc@5 99.650\n",
      "Epoch[158](100/196): Loss 0.0450 Prec@1(1,5) 98.43, 100.00\n",
      " * Acc@1 91.420 Acc@5 99.670\n",
      "Epoch[159](100/196): Loss 0.0434 Prec@1(1,5) 98.60, 99.99\n",
      " * Acc@1 91.450 Acc@5 99.700\n",
      "Epoch[160](100/196): Loss 0.0437 Prec@1(1,5) 98.50, 100.00\n",
      " * Acc@1 91.470 Acc@5 99.700\n",
      "Epoch[161](100/196): Loss 0.0409 Prec@1(1,5) 98.66, 100.00\n",
      " * Acc@1 91.480 Acc@5 99.670\n",
      "Epoch[162](100/196): Loss 0.0395 Prec@1(1,5) 98.67, 100.00\n",
      " * Acc@1 91.360 Acc@5 99.670\n",
      "Epoch[163](100/196): Loss 0.0398 Prec@1(1,5) 98.58, 99.99\n",
      " * Acc@1 91.380 Acc@5 99.690\n",
      "Epoch[164](100/196): Loss 0.0362 Prec@1(1,5) 98.75, 100.00\n",
      " * Acc@1 91.390 Acc@5 99.680\n",
      "Epoch[165](100/196): Loss 0.0340 Prec@1(1,5) 98.82, 100.00\n",
      " * Acc@1 91.260 Acc@5 99.640\n",
      "Epoch[166](100/196): Loss 0.0325 Prec@1(1,5) 98.84, 100.00\n",
      " * Acc@1 91.000 Acc@5 99.650\n",
      "Epoch[167](100/196): Loss 0.0293 Prec@1(1,5) 99.05, 100.00\n",
      " * Acc@1 91.220 Acc@5 99.680\n",
      "Epoch[168](100/196): Loss 0.0307 Prec@1(1,5) 98.97, 100.00\n",
      " * Acc@1 91.290 Acc@5 99.700\n",
      "Epoch[169](100/196): Loss 0.0295 Prec@1(1,5) 99.06, 100.00\n",
      " * Acc@1 91.160 Acc@5 99.700\n",
      "Epoch[170](100/196): Loss 0.0287 Prec@1(1,5) 99.03, 100.00\n",
      " * Acc@1 91.430 Acc@5 99.700\n",
      "Epoch[171](100/196): Loss 0.0294 Prec@1(1,5) 98.99, 100.00\n",
      " * Acc@1 91.190 Acc@5 99.680\n",
      "Epoch[172](100/196): Loss 0.0276 Prec@1(1,5) 99.07, 100.00\n",
      " * Acc@1 91.360 Acc@5 99.700\n",
      "Epoch[173](100/196): Loss 0.0253 Prec@1(1,5) 99.09, 100.00\n",
      " * Acc@1 91.390 Acc@5 99.630\n",
      "Epoch[174](100/196): Loss 0.0288 Prec@1(1,5) 98.98, 100.00\n",
      " * Acc@1 91.360 Acc@5 99.660\n",
      "Epoch[175](100/196): Loss 0.0262 Prec@1(1,5) 99.09, 100.00\n",
      " * Acc@1 91.370 Acc@5 99.670\n",
      "Epoch[176](100/196): Loss 0.0213 Prec@1(1,5) 99.28, 100.00\n",
      " * Acc@1 91.350 Acc@5 99.680\n",
      "Epoch[177](100/196): Loss 0.0246 Prec@1(1,5) 99.20, 100.00\n",
      " * Acc@1 91.230 Acc@5 99.680\n",
      "Epoch[178](100/196): Loss 0.0229 Prec@1(1,5) 99.23, 100.00\n",
      " * Acc@1 91.450 Acc@5 99.680\n",
      "Epoch[179](100/196): Loss 0.0221 Prec@1(1,5) 99.20, 100.00\n",
      " * Acc@1 91.410 Acc@5 99.670\n",
      "Epoch[180](100/196): Loss 0.0214 Prec@1(1,5) 99.30, 100.00\n",
      " * Acc@1 91.190 Acc@5 99.610\n",
      "Epoch[181](100/196): Loss 0.0222 Prec@1(1,5) 99.22, 100.00\n",
      " * Acc@1 91.360 Acc@5 99.680\n",
      "Epoch[182](100/196): Loss 0.0211 Prec@1(1,5) 99.28, 100.00\n",
      " * Acc@1 91.330 Acc@5 99.660\n",
      "Epoch[183](100/196): Loss 0.0188 Prec@1(1,5) 99.37, 100.00\n",
      " * Acc@1 91.310 Acc@5 99.620\n",
      "Epoch[184](100/196): Loss 0.0225 Prec@1(1,5) 99.27, 100.00\n",
      " * Acc@1 91.490 Acc@5 99.660\n",
      "Epoch[185](100/196): Loss 0.0211 Prec@1(1,5) 99.29, 100.00\n",
      " * Acc@1 91.250 Acc@5 99.670\n",
      "Epoch[186](100/196): Loss 0.0192 Prec@1(1,5) 99.33, 100.00\n",
      " * Acc@1 91.260 Acc@5 99.670\n",
      "Epoch[187](100/196): Loss 0.0182 Prec@1(1,5) 99.38, 100.00\n",
      " * Acc@1 91.260 Acc@5 99.640\n",
      "Epoch[188](100/196): Loss 0.0208 Prec@1(1,5) 99.29, 100.00\n",
      " * Acc@1 91.070 Acc@5 99.710\n",
      "Epoch[189](100/196): Loss 0.0179 Prec@1(1,5) 99.44, 100.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 91.530 Acc@5 99.680\n",
      "Epoch[190](100/196): Loss 0.0182 Prec@1(1,5) 99.38, 100.00\n",
      " * Acc@1 91.500 Acc@5 99.660\n",
      "Epoch[191](100/196): Loss 0.0172 Prec@1(1,5) 99.44, 100.00\n",
      " * Acc@1 91.350 Acc@5 99.750\n",
      "Epoch[192](100/196): Loss 0.0168 Prec@1(1,5) 99.43, 100.00\n",
      " * Acc@1 91.340 Acc@5 99.680\n",
      "Epoch[193](100/196): Loss 0.0167 Prec@1(1,5) 99.45, 100.00\n",
      " * Acc@1 91.350 Acc@5 99.580\n",
      "Epoch[194](100/196): Loss 0.0165 Prec@1(1,5) 99.44, 100.00\n",
      " * Acc@1 91.390 Acc@5 99.650\n",
      "Epoch[195](100/196): Loss 0.0169 Prec@1(1,5) 99.45, 100.00\n",
      " * Acc@1 91.260 Acc@5 99.660\n",
      "Epoch[196](100/196): Loss 0.0191 Prec@1(1,5) 99.39, 100.00\n",
      " * Acc@1 91.460 Acc@5 99.660\n",
      "Epoch[197](100/196): Loss 0.0166 Prec@1(1,5) 99.44, 100.00\n",
      " * Acc@1 91.320 Acc@5 99.620\n",
      "Epoch[198](100/196): Loss 0.0180 Prec@1(1,5) 99.36, 100.00\n",
      " * Acc@1 91.370 Acc@5 99.640\n",
      "Epoch[199](100/196): Loss 0.0151 Prec@1(1,5) 99.52, 100.00\n",
      " * Acc@1 91.520 Acc@5 99.690\n",
      "Epoch[200](100/196): Loss 0.0143 Prec@1(1,5) 99.56, 100.00\n",
      " * Acc@1 91.330 Acc@5 99.660\n",
      "Epoch[201](100/196): Loss 0.0144 Prec@1(1,5) 99.50, 100.00\n",
      " * Acc@1 91.110 Acc@5 99.670\n",
      "Epoch[202](100/196): Loss 0.0172 Prec@1(1,5) 99.40, 100.00\n",
      " * Acc@1 91.230 Acc@5 99.710\n",
      "Epoch[203](100/196): Loss 0.0165 Prec@1(1,5) 99.50, 100.00\n",
      " * Acc@1 91.400 Acc@5 99.650\n",
      "Epoch[204](100/196): Loss 0.0151 Prec@1(1,5) 99.49, 100.00\n",
      " * Acc@1 91.540 Acc@5 99.610\n",
      "Epoch[205](100/196): Loss 0.0163 Prec@1(1,5) 99.45, 100.00\n",
      " * Acc@1 91.420 Acc@5 99.610\n",
      "Epoch[206](100/196): Loss 0.0145 Prec@1(1,5) 99.49, 100.00\n",
      " * Acc@1 91.370 Acc@5 99.610\n",
      "Epoch[207](100/196): Loss 0.0160 Prec@1(1,5) 99.44, 100.00\n",
      " * Acc@1 91.480 Acc@5 99.620\n",
      "Epoch[208](100/196): Loss 0.0145 Prec@1(1,5) 99.50, 100.00\n",
      " * Acc@1 91.470 Acc@5 99.600\n",
      "Epoch[209](100/196): Loss 0.0147 Prec@1(1,5) 99.52, 100.00\n",
      " * Acc@1 91.150 Acc@5 99.540\n",
      "Epoch[210](100/196): Loss 0.0154 Prec@1(1,5) 99.48, 100.00\n",
      " * Acc@1 91.350 Acc@5 99.600\n",
      "Epoch[211](100/196): Loss 0.0128 Prec@1(1,5) 99.56, 100.00\n",
      " * Acc@1 91.200 Acc@5 99.620\n",
      "Epoch[212](100/196): Loss 0.0147 Prec@1(1,5) 99.54, 100.00\n",
      " * Acc@1 91.410 Acc@5 99.670\n",
      "Epoch[213](100/196): Loss 0.0146 Prec@1(1,5) 99.56, 100.00\n",
      " * Acc@1 91.300 Acc@5 99.670\n",
      "Epoch[214](100/196): Loss 0.0144 Prec@1(1,5) 99.48, 100.00\n",
      " * Acc@1 91.500 Acc@5 99.610\n",
      "Epoch[215](100/196): Loss 0.0140 Prec@1(1,5) 99.49, 100.00\n",
      " * Acc@1 91.430 Acc@5 99.640\n",
      "Epoch[216](100/196): Loss 0.0150 Prec@1(1,5) 99.50, 100.00\n",
      " * Acc@1 91.440 Acc@5 99.620\n",
      "Epoch[217](100/196): Loss 0.0135 Prec@1(1,5) 99.52, 100.00\n",
      " * Acc@1 91.340 Acc@5 99.680\n",
      "Epoch[218](100/196): Loss 0.0134 Prec@1(1,5) 99.58, 100.00\n",
      " * Acc@1 91.240 Acc@5 99.660\n",
      "Epoch[219](100/196): Loss 0.0146 Prec@1(1,5) 99.50, 100.00\n",
      " * Acc@1 91.110 Acc@5 99.620\n",
      "Epoch[220](100/196): Loss 0.0155 Prec@1(1,5) 99.50, 100.00\n",
      " * Acc@1 91.070 Acc@5 99.660\n",
      "Epoch[221](100/196): Loss 0.0136 Prec@1(1,5) 99.54, 100.00\n",
      " * Acc@1 91.160 Acc@5 99.630\n",
      "Epoch[222](100/196): Loss 0.0128 Prec@1(1,5) 99.57, 100.00\n",
      " * Acc@1 91.200 Acc@5 99.670\n",
      "Epoch[223](100/196): Loss 0.0142 Prec@1(1,5) 99.53, 100.00\n",
      " * Acc@1 91.260 Acc@5 99.640\n",
      "Epoch[224](100/196): Loss 0.0132 Prec@1(1,5) 99.60, 100.00\n",
      " * Acc@1 91.310 Acc@5 99.560\n",
      "Epoch[225](100/196): Loss 0.0127 Prec@1(1,5) 99.55, 100.00\n",
      " * Acc@1 91.510 Acc@5 99.640\n",
      "Epoch[226](100/196): Loss 0.0094 Prec@1(1,5) 99.71, 100.00\n",
      " * Acc@1 91.490 Acc@5 99.630\n",
      "Epoch[227](100/196): Loss 0.0084 Prec@1(1,5) 99.74, 100.00\n",
      " * Acc@1 91.480 Acc@5 99.620\n",
      "Epoch[228](100/196): Loss 0.0078 Prec@1(1,5) 99.75, 100.00\n",
      " * Acc@1 91.560 Acc@5 99.630\n",
      "Epoch[229](100/196): Loss 0.0091 Prec@1(1,5) 99.73, 100.00\n",
      " * Acc@1 91.560 Acc@5 99.620\n",
      "Epoch[230](100/196): Loss 0.0072 Prec@1(1,5) 99.81, 100.00\n",
      " * Acc@1 91.650 Acc@5 99.630\n",
      "Epoch[231](100/196): Loss 0.0074 Prec@1(1,5) 99.78, 100.00\n",
      " * Acc@1 91.650 Acc@5 99.660\n",
      "Epoch[232](100/196): Loss 0.0071 Prec@1(1,5) 99.80, 100.00\n",
      " * Acc@1 91.650 Acc@5 99.610\n",
      "Epoch[233](100/196): Loss 0.0077 Prec@1(1,5) 99.78, 100.00\n",
      " * Acc@1 91.670 Acc@5 99.620\n",
      "Epoch[234](100/196): Loss 0.0076 Prec@1(1,5) 99.76, 100.00\n",
      " * Acc@1 91.670 Acc@5 99.670\n",
      "Epoch[235](100/196): Loss 0.0080 Prec@1(1,5) 99.76, 100.00\n",
      " * Acc@1 91.680 Acc@5 99.620\n",
      "Epoch[236](100/196): Loss 0.0062 Prec@1(1,5) 99.81, 100.00\n",
      " * Acc@1 91.730 Acc@5 99.630\n",
      "Epoch[237](100/196): Loss 0.0075 Prec@1(1,5) 99.75, 100.00\n",
      " * Acc@1 91.670 Acc@5 99.660\n",
      "Epoch[238](100/196): Loss 0.0062 Prec@1(1,5) 99.84, 100.00\n",
      " * Acc@1 91.620 Acc@5 99.640\n",
      "Epoch[239](100/196): Loss 0.0062 Prec@1(1,5) 99.82, 100.00\n",
      " * Acc@1 91.700 Acc@5 99.610\n",
      "Epoch[240](100/196): Loss 0.0065 Prec@1(1,5) 99.79, 100.00\n",
      " * Acc@1 91.660 Acc@5 99.640\n",
      "Epoch[241](100/196): Loss 0.0054 Prec@1(1,5) 99.85, 100.00\n",
      " * Acc@1 91.660 Acc@5 99.640\n",
      "Epoch[242](100/196): Loss 0.0060 Prec@1(1,5) 99.82, 100.00\n",
      " * Acc@1 91.580 Acc@5 99.630\n",
      "Epoch[243](100/196): Loss 0.0062 Prec@1(1,5) 99.79, 100.00\n",
      " * Acc@1 91.660 Acc@5 99.610\n",
      "Epoch[244](100/196): Loss 0.0057 Prec@1(1,5) 99.83, 100.00\n",
      " * Acc@1 91.710 Acc@5 99.600\n",
      "Epoch[245](100/196): Loss 0.0051 Prec@1(1,5) 99.88, 100.00\n",
      " * Acc@1 91.740 Acc@5 99.620\n",
      "Epoch[246](100/196): Loss 0.0062 Prec@1(1,5) 99.81, 100.00\n",
      " * Acc@1 91.570 Acc@5 99.590\n",
      "Epoch[247](100/196): Loss 0.0068 Prec@1(1,5) 99.80, 100.00\n",
      " * Acc@1 91.700 Acc@5 99.640\n",
      "Epoch[248](100/196): Loss 0.0052 Prec@1(1,5) 99.86, 100.00\n",
      " * Acc@1 91.640 Acc@5 99.620\n",
      "Epoch[249](100/196): Loss 0.0063 Prec@1(1,5) 99.81, 100.00\n",
      " * Acc@1 91.590 Acc@5 99.670\n",
      "Epoch[250](100/196): Loss 0.0051 Prec@1(1,5) 99.86, 100.00\n",
      " * Acc@1 91.570 Acc@5 99.600\n",
      "Epoch[251](100/196): Loss 0.0050 Prec@1(1,5) 99.84, 100.00\n",
      " * Acc@1 91.630 Acc@5 99.590\n",
      "Epoch[252](100/196): Loss 0.0055 Prec@1(1,5) 99.83, 100.00\n",
      " * Acc@1 91.480 Acc@5 99.560\n",
      "Epoch[253](100/196): Loss 0.0055 Prec@1(1,5) 99.84, 100.00\n",
      " * Acc@1 91.650 Acc@5 99.610\n",
      "Epoch[254](100/196): Loss 0.0061 Prec@1(1,5) 99.82, 100.00\n",
      " * Acc@1 91.650 Acc@5 99.600\n",
      "Epoch[255](100/196): Loss 0.0061 Prec@1(1,5) 99.84, 100.00\n",
      " * Acc@1 91.690 Acc@5 99.600\n",
      "Epoch[256](100/196): Loss 0.0057 Prec@1(1,5) 99.83, 100.00\n",
      " * Acc@1 91.580 Acc@5 99.600\n",
      "Epoch[257](100/196): Loss 0.0053 Prec@1(1,5) 99.85, 100.00\n",
      " * Acc@1 91.550 Acc@5 99.590\n",
      "Epoch[258](100/196): Loss 0.0048 Prec@1(1,5) 99.85, 100.00\n",
      " * Acc@1 91.620 Acc@5 99.620\n",
      "Epoch[259](100/196): Loss 0.0061 Prec@1(1,5) 99.82, 100.00\n",
      " * Acc@1 91.540 Acc@5 99.590\n",
      "Epoch[260](100/196): Loss 0.0061 Prec@1(1,5) 99.81, 100.00\n",
      " * Acc@1 91.710 Acc@5 99.640\n",
      "Epoch[261](100/196): Loss 0.0057 Prec@1(1,5) 99.83, 100.00\n",
      " * Acc@1 91.680 Acc@5 99.620\n",
      "Epoch[262](100/196): Loss 0.0058 Prec@1(1,5) 99.78, 100.00\n",
      " * Acc@1 91.690 Acc@5 99.630\n",
      "Epoch[263](100/196): Loss 0.0057 Prec@1(1,5) 99.82, 100.00\n",
      " * Acc@1 91.720 Acc@5 99.620\n",
      "Epoch[264](100/196): Loss 0.0049 Prec@1(1,5) 99.86, 100.00\n",
      " * Acc@1 91.690 Acc@5 99.620\n",
      "Epoch[265](100/196): Loss 0.0052 Prec@1(1,5) 99.86, 100.00\n",
      " * Acc@1 91.760 Acc@5 99.610\n",
      "Epoch[266](100/196): Loss 0.0058 Prec@1(1,5) 99.81, 100.00\n",
      " * Acc@1 91.660 Acc@5 99.620\n",
      "Epoch[267](100/196): Loss 0.0049 Prec@1(1,5) 99.85, 100.00\n",
      " * Acc@1 91.590 Acc@5 99.620\n",
      "Epoch[268](100/196): Loss 0.0050 Prec@1(1,5) 99.85, 100.00\n",
      " * Acc@1 91.640 Acc@5 99.670\n",
      "Epoch[269](100/196): Loss 0.0049 Prec@1(1,5) 99.87, 100.00\n",
      " * Acc@1 91.540 Acc@5 99.600\n",
      "Epoch[270](100/196): Loss 0.0050 Prec@1(1,5) 99.88, 100.00\n",
      " * Acc@1 91.640 Acc@5 99.630\n",
      "Epoch[271](100/196): Loss 0.0053 Prec@1(1,5) 99.81, 100.00\n",
      " * Acc@1 91.620 Acc@5 99.640\n",
      "Epoch[272](100/196): Loss 0.0046 Prec@1(1,5) 99.89, 100.00\n",
      " * Acc@1 91.600 Acc@5 99.640\n",
      "Epoch[273](100/196): Loss 0.0057 Prec@1(1,5) 99.84, 100.00\n",
      " * Acc@1 91.600 Acc@5 99.650\n",
      "Epoch[274](100/196): Loss 0.0054 Prec@1(1,5) 99.83, 100.00\n",
      " * Acc@1 91.740 Acc@5 99.630\n",
      "Epoch[275](100/196): Loss 0.0046 Prec@1(1,5) 99.89, 100.00\n",
      " * Acc@1 91.670 Acc@5 99.580\n",
      "Epoch[276](100/196): Loss 0.0051 Prec@1(1,5) 99.86, 100.00\n",
      " * Acc@1 91.650 Acc@5 99.640\n",
      "Epoch[277](100/196): Loss 0.0043 Prec@1(1,5) 99.86, 100.00\n",
      " * Acc@1 91.530 Acc@5 99.590\n",
      "Epoch[278](100/196): Loss 0.0041 Prec@1(1,5) 99.89, 100.00\n",
      " * Acc@1 91.760 Acc@5 99.590\n",
      "Epoch[279](100/196): Loss 0.0044 Prec@1(1,5) 99.86, 100.00\n",
      " * Acc@1 91.610 Acc@5 99.560\n",
      "Epoch[280](100/196): Loss 0.0047 Prec@1(1,5) 99.89, 100.00\n",
      " * Acc@1 91.550 Acc@5 99.570\n",
      "Epoch[281](100/196): Loss 0.0047 Prec@1(1,5) 99.85, 100.00\n",
      " * Acc@1 91.650 Acc@5 99.570\n",
      "Epoch[282](100/196): Loss 0.0040 Prec@1(1,5) 99.90, 100.00\n",
      " * Acc@1 91.690 Acc@5 99.630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[283](100/196): Loss 0.0042 Prec@1(1,5) 99.88, 100.00\n",
      " * Acc@1 91.610 Acc@5 99.620\n",
      "Epoch[284](100/196): Loss 0.0047 Prec@1(1,5) 99.85, 100.00\n",
      " * Acc@1 91.660 Acc@5 99.590\n",
      "Epoch[285](100/196): Loss 0.0054 Prec@1(1,5) 99.82, 100.00\n",
      " * Acc@1 91.530 Acc@5 99.640\n",
      "Epoch[286](100/196): Loss 0.0040 Prec@1(1,5) 99.88, 100.00\n",
      " * Acc@1 91.660 Acc@5 99.620\n",
      "Epoch[287](100/196): Loss 0.0040 Prec@1(1,5) 99.90, 100.00\n",
      " * Acc@1 91.660 Acc@5 99.590\n",
      "Epoch[288](100/196): Loss 0.0051 Prec@1(1,5) 99.83, 100.00\n",
      " * Acc@1 91.730 Acc@5 99.570\n",
      "Epoch[289](100/196): Loss 0.0045 Prec@1(1,5) 99.88, 100.00\n",
      " * Acc@1 91.740 Acc@5 99.590\n",
      "Epoch[290](100/196): Loss 0.0039 Prec@1(1,5) 99.88, 100.00\n",
      " * Acc@1 91.810 Acc@5 99.590\n",
      "Epoch[291](100/196): Loss 0.0040 Prec@1(1,5) 99.90, 100.00\n",
      " * Acc@1 91.690 Acc@5 99.590\n",
      "Epoch[292](100/196): Loss 0.0047 Prec@1(1,5) 99.86, 100.00\n",
      " * Acc@1 91.750 Acc@5 99.600\n",
      "Epoch[293](100/196): Loss 0.0045 Prec@1(1,5) 99.88, 100.00\n",
      " * Acc@1 91.790 Acc@5 99.570\n",
      "Epoch[294](100/196): Loss 0.0048 Prec@1(1,5) 99.86, 100.00\n",
      " * Acc@1 91.770 Acc@5 99.580\n",
      "Epoch[295](100/196): Loss 0.0059 Prec@1(1,5) 99.81, 100.00\n",
      " * Acc@1 91.750 Acc@5 99.580\n",
      "Epoch[296](100/196): Loss 0.0043 Prec@1(1,5) 99.87, 100.00\n",
      " * Acc@1 91.730 Acc@5 99.620\n",
      "Epoch[297](100/196): Loss 0.0039 Prec@1(1,5) 99.89, 100.00\n",
      " * Acc@1 91.630 Acc@5 99.540\n",
      "Epoch[298](100/196): Loss 0.0041 Prec@1(1,5) 99.87, 100.00\n",
      " * Acc@1 91.770 Acc@5 99.570\n",
      "Epoch[299](100/196): Loss 0.0043 Prec@1(1,5) 99.88, 100.00\n",
      " * Acc@1 91.640 Acc@5 99.620\n",
      "Epoch[300](100/196): Loss 0.0042 Prec@1(1,5) 99.87, 100.00\n",
      " * Acc@1 91.580 Acc@5 99.600\n",
      "Epoch[301](100/196): Loss 0.0036 Prec@1(1,5) 99.90, 100.00\n",
      " * Acc@1 91.660 Acc@5 99.610\n",
      "Epoch[302](100/196): Loss 0.0044 Prec@1(1,5) 99.86, 100.00\n",
      " * Acc@1 91.790 Acc@5 99.590\n",
      "Epoch[303](100/196): Loss 0.0041 Prec@1(1,5) 99.86, 100.00\n",
      " * Acc@1 91.690 Acc@5 99.590\n",
      "Epoch[304](100/196): Loss 0.0046 Prec@1(1,5) 99.87, 100.00\n",
      " * Acc@1 91.730 Acc@5 99.600\n",
      "Epoch[305](100/196): Loss 0.0054 Prec@1(1,5) 99.83, 100.00\n",
      " * Acc@1 91.670 Acc@5 99.600\n",
      "Epoch[306](100/196): Loss 0.0048 Prec@1(1,5) 99.86, 100.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-548b74c638e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-09259755be2b>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(epoch, net, i)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-974f9f7f32a4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-974f9f7f32a4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 350\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch,1,net,optimizer,scheduler)\n",
    "    test(epoch,net,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
