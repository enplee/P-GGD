{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:50:11.454649Z",
     "start_time": "2020-11-02T00:50:10.904145Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import collections\n",
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import copy\n",
    "from thop import profile\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "import torch.utils\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:50:11.768936Z",
     "start_time": "2020-11-02T00:50:11.750920Z"
    },
    "code_folding": [
     4,
     11,
     18,
     23,
     64
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pdb\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "def make_divisible(x, divisible_by=8):\n",
    "    import numpy as np\n",
    "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, compress_rate, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t-ex, c-channel, n-blocknum, s-stride\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 1], # NOTE: change stride 2 -> 1 for CIFAR10\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "        self.compress_rate=compress_rate[:]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        # input_channel = make_divisible(input_channel * width_mult)  # first channel is always 32!\n",
    "        self.last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        cnt=1\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = make_divisible(c * width_mult) if t > 1 else c\n",
    "            output_channel = int((1-self.compress_rate[cnt])*output_channel)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "            cnt+=1\n",
    "\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        #self.classifier = nn.Linear(self.last_channel, n_class)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "def mobilenet_v2(compress_rate,n_class=10):\n",
    "    model = MobileNetV2(compress_rate=compress_rate,n_class=n_class,width_mult=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:50:12.651745Z",
     "start_time": "2020-11-02T00:50:12.628724Z"
    },
    "code_folding": [
     1,
     29,
     34,
     39,
     48,
     97,
     163,
     166
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare resNet_50 model...\n"
     ]
    }
   ],
   "source": [
    "print(\"prepare resNet_50 model...\")\n",
    "def adapt_channel(compress_rate, num_layers):\n",
    "\n",
    "    if num_layers==56:\n",
    "        stage_repeat = [9, 9, 9]\n",
    "        stage_out_channel = [16] + [16] * 9 + [32] * 9 + [64] * 9\n",
    "    elif num_layers==110:\n",
    "        stage_repeat = [18, 18, 18]\n",
    "        stage_out_channel = [16] + [16] * 18 + [32] * 18 + [64] * 18\n",
    "\n",
    "    stage_oup_cprate = []\n",
    "    stage_oup_cprate += [compress_rate[0]]\n",
    "    for i in range(len(stage_repeat)-1):\n",
    "        stage_oup_cprate += [compress_rate[i+1]] * stage_repeat[i]\n",
    "    stage_oup_cprate +=[0.] * stage_repeat[-1]\n",
    "    mid_cprate = compress_rate[len(stage_repeat):]\n",
    "\n",
    "    overall_channel = []\n",
    "    mid_channel = []\n",
    "    for i in range(len(stage_out_channel)):\n",
    "        if i == 0 :\n",
    "            overall_channel += [int(stage_out_channel[i] * (1-stage_oup_cprate[i]))]\n",
    "        else:\n",
    "            overall_channel += [int(stage_out_channel[i] * (1-stage_oup_cprate[i]))]\n",
    "            mid_channel += [int(stage_out_channel[i] * (1-mid_cprate[i-1]))]\n",
    "\n",
    "    return overall_channel, mid_channel\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, midplanes, inplanes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.planes = planes\n",
    "        self.conv1 = conv3x3(inplanes, midplanes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(midplanes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = conv3x3(midplanes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.stride = stride\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inplanes != planes:\n",
    "            if stride!=1:\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(x[:, :, ::2, ::2],\n",
    "                                    (0, 0, 0, 0, (planes-inplanes)//2, planes-inplanes-(planes-inplanes)//2), \"constant\", 0))\n",
    "            else:\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(x[:, :, :, :],\n",
    "                                    (0, 0, 0, 0, (planes-inplanes)//2, planes-inplanes-(planes-inplanes)//2), \"constant\", 0))\n",
    "            #self.shortcut = LambdaLayer(\n",
    "            #    lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4),\"constant\", 0))\n",
    "\n",
    "            '''self.shortcut = nn.Sequential(\n",
    "                conv1x1(inplanes, planes, stride=stride),\n",
    "                #nn.BatchNorm2d(planes),\n",
    "            )#'''\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        #print(self.stride, self.inplanes, self.planes, out.size(), self.shortcut(x).size())\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_layers, compress_rate, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert (num_layers - 2) % 6 == 0, 'depth should be 6n+2'\n",
    "        n = (num_layers - 2) // 6\n",
    "\n",
    "        self.num_layer = num_layers\n",
    "        self.overall_channel, self.mid_channel = adapt_channel(compress_rate, num_layers)\n",
    "\n",
    "        self.layer_num = 0\n",
    "        self.conv1 = nn.Conv2d(3, self.overall_channel[self.layer_num], kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.overall_channel[self.layer_num])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_num += 1\n",
    "\n",
    "        #self.layers = nn.ModuleList()\n",
    "        self.layer1 = self._make_layer(block, blocks_num=n, stride=1)\n",
    "        self.layer2 = self._make_layer(block, blocks_num=n, stride=2)\n",
    "        self.layer3 = self._make_layer(block, blocks_num=n, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        if self.num_layer == 56:\n",
    "            self.fc = nn.Linear(64 * BasicBlock.expansion, num_classes)\n",
    "        else:\n",
    "            self.linear = nn.Linear(64 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, blocks_num, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.mid_channel[self.layer_num - 1], self.overall_channel[self.layer_num - 1],\n",
    "                                 self.overall_channel[self.layer_num], stride))\n",
    "        self.layer_num += 1\n",
    "\n",
    "        for i in range(1, blocks_num):\n",
    "            layers.append(block(self.mid_channel[self.layer_num - 1], self.overall_channel[self.layer_num - 1],\n",
    "                                     self.overall_channel[self.layer_num]))\n",
    "            self.layer_num += 1\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        for i, block in enumerate(self.layer1):\n",
    "            x = block(x)\n",
    "        for i, block in enumerate(self.layer2):\n",
    "            x = block(x)\n",
    "        for i, block in enumerate(self.layer3):\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.num_layer == 56:\n",
    "            x = self.fc(x)\n",
    "        else:\n",
    "            x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "def resnet_56(compress_rate):\n",
    "    return ResNet(BasicBlock, 56, compress_rate=compress_rate)\n",
    "def resnet_110(compress_rate):\n",
    "    return ResNet(BasicBlock, 110, compress_rate=compress_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:50:13.557575Z",
     "start_time": "2020-11-02T00:50:13.527548Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "cudnn.benchmark = True\n",
    "cudnn.enabled=True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T04:56:26.597332Z",
     "start_time": "2020-10-07T04:56:26.593328Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:50:17.314016Z",
     "start_time": "2020-11-02T00:50:17.262970Z"
    },
    "code_folding": [
     0,
     107,
     138
    ]
   },
   "outputs": [],
   "source": [
    "def load_mobilenetv2_model(model, oristate_dict):\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    last_select_index = None\n",
    "\n",
    "    all_honey_conv_weight = []\n",
    "\n",
    "    bn_part_name=['.weight','.bias','.running_mean','.running_var']\n",
    "    prefix = rank_conv_prefix+'rank_conv'\n",
    "    subfix = \".npy\"\n",
    "\n",
    "    layer_cnt=1\n",
    "    conv_cnt=1\n",
    "    cfg=[1,2,3,4,3,3,1,1]\n",
    "    for layer, num in enumerate(cfg):\n",
    "        if layer_cnt==1:\n",
    "            conv_id=[0,3]\n",
    "        elif layer_cnt==18:\n",
    "            conv_id=[0]\n",
    "        else:\n",
    "            conv_id=[0,3,6]\n",
    "\n",
    "        for k in range(num):\n",
    "            if layer_cnt==18:\n",
    "                block_name = 'features.' + str(layer_cnt) + '.'\n",
    "            else:\n",
    "                block_name = 'features.'+str(layer_cnt)+'.conv.'\n",
    "\n",
    "            for l in conv_id:\n",
    "                conv_cnt += 1\n",
    "                conv_name = block_name + str(l)\n",
    "                bn_name = block_name + str(l+1)\n",
    "\n",
    "                conv_weight_name = conv_name + '.weight'\n",
    "                all_honey_conv_weight.append(conv_weight_name)\n",
    "                oriweight = oristate_dict[conv_weight_name]\n",
    "                curweight = state_dict[name_base+conv_weight_name]\n",
    "                orifilter_num = oriweight.size(0)\n",
    "                currentfilter_num = curweight.size(0)\n",
    "\n",
    "                if orifilter_num != currentfilter_num:\n",
    "                    print('loading rank from: ' + prefix + str(conv_cnt) + subfix)\n",
    "                    rank = np.load(prefix + str(conv_cnt) + subfix)\n",
    "                    select_index = np.argsort(rank)[orifilter_num - currentfilter_num:]  # preserved filter id\n",
    "                    select_index.sort()\n",
    "\n",
    "                    if (l==6 or (l==0 and layer_cnt!=1) or (l==3 and layer_cnt==1)) and last_select_index is not None:\n",
    "                        for index_i, i in enumerate(select_index):\n",
    "                            for index_j, j in enumerate(last_select_index):\n",
    "                                state_dict[name_base+conv_weight_name][index_i][index_j] = \\\n",
    "                                    oristate_dict[conv_weight_name][i][j]\n",
    "                            for bn_part in bn_part_name:\n",
    "                                state_dict[name_base + bn_name + bn_part][index_i] = \\\n",
    "                                    oristate_dict[bn_name + bn_part][i]\n",
    "                    else:\n",
    "                        for index_i, i in enumerate(select_index):\n",
    "                            state_dict[name_base+conv_weight_name][index_i] = \\\n",
    "                                oristate_dict[conv_weight_name][i]\n",
    "                            for bn_part in bn_part_name:\n",
    "                                state_dict[name_base + bn_name + bn_part][index_i] = \\\n",
    "                                    oristate_dict[bn_name + bn_part][i]\n",
    "\n",
    "                    last_select_index = select_index\n",
    "\n",
    "                elif  (l==6 or (l==0 and layer_cnt!=1) or (l==3 and layer_cnt==1)) and last_select_index is not None:\n",
    "                    for index_i in range(orifilter_num):\n",
    "                        for index_j, j in enumerate(last_select_index):\n",
    "                            state_dict[name_base+conv_weight_name][index_i][index_j] = \\\n",
    "                                oristate_dict[conv_weight_name][index_i][j]\n",
    "                    for bn_part in bn_part_name:\n",
    "                        state_dict[name_base + bn_name + bn_part] = \\\n",
    "                            oristate_dict[bn_name + bn_part]\n",
    "                    last_select_index = None\n",
    "\n",
    "                else:\n",
    "                    state_dict[name_base+conv_weight_name] = oriweight\n",
    "                    for bn_part in bn_part_name:\n",
    "                        state_dict[name_base + bn_name + bn_part] = \\\n",
    "                            oristate_dict[bn_name + bn_part]\n",
    "                    last_select_index = None\n",
    "\n",
    "                state_dict[name_base + bn_name + '.num_batches_tracked'] = oristate_dict[bn_name + '.num_batches_tracked']\n",
    "\n",
    "            layer_cnt+=1\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        name = name.replace('module.', '')\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            conv_name = name + '.weight'\n",
    "            bn_name = list(name[:])\n",
    "            bn_name[-1] = str(int(name[-1])+1)\n",
    "            bn_name = ''.join(bn_name)\n",
    "            if conv_name not in all_honey_conv_weight:\n",
    "                state_dict[name_base+conv_name] = oristate_dict[conv_name]\n",
    "                for bn_part in bn_part_name:\n",
    "                    state_dict[name_base + bn_name + bn_part] = \\\n",
    "                        oristate_dict[bn_name + bn_part]\n",
    "                state_dict[name_base + bn_name + '.num_batches_tracked'] = oristate_dict[bn_name + '.num_batches_tracked']\n",
    "\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            state_dict[name_base+name + '.weight'] = oristate_dict[name + '.weight']\n",
    "            state_dict[name_base+name + '.bias'] = oristate_dict[name + '.bias']\n",
    "\n",
    "    model.load_state_dict(state_dict,strict=False)\n",
    "    print(\"finish pruning\")\n",
    "    \n",
    "def adjust_learning_rate(optimizer, epoch, step, len_iter):\n",
    "\n",
    "    if lr_type == 'step':\n",
    "        factor = epoch // 30\n",
    "        if epoch >= 80:\n",
    "            factor = factor + 1\n",
    "        lr = learning_rate * (0.1 ** factor)\n",
    "\n",
    "    elif lr_type == 'cos':  # cos without warm-up\n",
    "        lr = 0.5 * learning_rate * (1 + math.cos(math.pi * (epoch - 5) / (epochs - 5)))\n",
    "\n",
    "    elif lr_type == 'exp':\n",
    "        step = 1\n",
    "        decay = 0.96\n",
    "        lr = learning_rate * (decay ** (epoch // step))\n",
    "\n",
    "    elif lr_type == 'fixed':\n",
    "        lr = learning_rate\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    #Warmup\n",
    "    if epoch < 5:\n",
    "        lr = lr * float(1 + step + epoch * len_iter) / (5. * len_iter)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if step == 0:\n",
    "        print('learning_rate: ' + str(lr))\n",
    "\n",
    "def load_resnet_model(model, oristate_dict, layer):\n",
    "    cfg = {\n",
    "        56: [9, 9, 9],\n",
    "        110: [18, 18, 18],\n",
    "    }\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    current_cfg = cfg[layer]\n",
    "    last_select_index = None\n",
    "\n",
    "    all_conv_weight = []\n",
    "\n",
    "    prefix = rank_conv_prefix+'rank_conv'\n",
    "    subfix = \".npy\"\n",
    "\n",
    "    cnt=1\n",
    "    for layer, num in enumerate(current_cfg):\n",
    "        layer_name = 'layer' + str(layer + 1) + '.'\n",
    "        for k in range(num):\n",
    "            for l in range(2):\n",
    "\n",
    "                cnt+=1\n",
    "                cov_id=cnt\n",
    "\n",
    "                conv_name = layer_name + str(k) + '.conv' + str(l + 1)\n",
    "                conv_weight_name = conv_name + '.weight'\n",
    "                all_conv_weight.append(conv_weight_name)\n",
    "                oriweight = oristate_dict[conv_weight_name]\n",
    "                curweight =state_dict[name_base+conv_weight_name]\n",
    "                orifilter_num = oriweight.size(0)\n",
    "                currentfilter_num = curweight.size(0)\n",
    "\n",
    "                if orifilter_num != currentfilter_num:\n",
    "                    print('loading rank from: ' + prefix + str(cov_id) + subfix)\n",
    "                    rank = np.load(prefix + str(cov_id) + subfix)\n",
    "                    select_index = np.argsort(rank)[orifilter_num - currentfilter_num:]  # preserved filter id\n",
    "                    select_index.sort()\n",
    "\n",
    "                    if last_select_index is not None:\n",
    "                        for index_i, i in enumerate(select_index):\n",
    "                            for index_j, j in enumerate(last_select_index):\n",
    "                                state_dict[name_base+conv_weight_name][index_i][index_j] = \\\n",
    "                                    oristate_dict[conv_weight_name][i][j]\n",
    "                    else:\n",
    "                        for index_i, i in enumerate(select_index):\n",
    "                            state_dict[name_base+conv_weight_name][index_i] = \\\n",
    "                                oristate_dict[conv_weight_name][i]\n",
    "\n",
    "                    last_select_index = select_index\n",
    "\n",
    "                elif last_select_index is not None:\n",
    "                    for index_i in range(orifilter_num):\n",
    "                        for index_j, j in enumerate(last_select_index):\n",
    "                            state_dict[name_base+conv_weight_name][index_i][index_j] = \\\n",
    "                                oristate_dict[conv_weight_name][index_i][j]\n",
    "                    last_select_index = None\n",
    "\n",
    "                else:\n",
    "                    state_dict[name_base+conv_weight_name] = oriweight\n",
    "                    last_select_index = None\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        name = name.replace('module.', '')\n",
    "\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            conv_name = name + '.weight'\n",
    "            if 'shortcut' in name:\n",
    "                continue\n",
    "            if conv_name not in all_conv_weight:\n",
    "                state_dict[name_base+conv_name] = oristate_dict[conv_name]\n",
    "\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            state_dict[name_base+name + '.weight'] = oristate_dict[name + '.weight']\n",
    "            state_dict[name_base+name + '.bias'] = oristate_dict[name + '.bias']\n",
    "\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T00:50:18.346963Z",
     "start_time": "2020-11-02T00:50:18.342959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "超参数\n",
      "PruneGraft_cifar10_press2_mobilenet_v2\n"
     ]
    }
   ],
   "source": [
    "print(\"超参数\")\n",
    "CLASSES = 10\n",
    "lr_type = 'step'\n",
    "epochs = 400\n",
    "batch_size=  256\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "# 0.006\n",
    "weight_decay = 0.005\n",
    "lr_decay_step = '150,225'\n",
    "\n",
    "best_acc = 0\n",
    "# \"mobilenet_v2\" \"resnet_56\"\n",
    "arch = \"mobilenet_v2\"\n",
    "# \"[0.]+[0.4]*2+[0.5]*9+[0.6]*9+[0.7]*9\" “[0.]+[0.15]*2+[0.4]*27”\"'[0.]+[0.18]*29'\"\n",
    "compress_rate = \"[0.]+[0.3]*7\"\n",
    "# \"./data/model/Hrank_preTrain/cifar-10/resnet_56.pt.pt\"\n",
    "pretrain_dir = \"./data/model/Hrank_preTrain/cifar-10/resnet_56.pt.pt\"\n",
    "save_dir = \"PruneGraft_cifar10_press2_\"+arch\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T07:00:26.526121Z",
     "start_time": "2020-11-01T07:00:26.525121Z"
    }
   },
   "outputs": [],
   "source": [
    "# batch_size: 256\n",
    "# epochs: 300\n",
    "# learning_rate: 0.01\n",
    "# lr_decay_step: 150,225\n",
    "# momentum: 0.9\n",
    "# weight_decay: 0.006\n",
    "# compress_rate: [0.]+[0.4]*2+[0.5]*9+[0.6]*9+[0.7]*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T07:03:03.538919Z",
     "start_time": "2020-11-01T07:03:03.533915Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare compress_rate\n",
      "compress_rate:[0.0, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n"
     ]
    }
   ],
   "source": [
    "start_t = time.time()\n",
    "print(\"prepare compress_rate\")\n",
    "def process_compress_rate(compress_rate):\n",
    "    if compress_rate:# 处理args.compress_rate\n",
    "        import re\n",
    "        cprate_str = compress_rate\n",
    "        cprate_str_list = cprate_str.split('+')\n",
    "        pat_cprate = re.compile(r'\\d+\\.\\d*')\n",
    "        pat_num = re.compile(r'\\*\\d+')\n",
    "        cprate = []\n",
    "        for x in cprate_str_list:\n",
    "            num = 1\n",
    "            find_num = re.findall(pat_num, x)\n",
    "            if find_num:\n",
    "                assert len(find_num) == 1\n",
    "                num = int(find_num[0].replace('*', ''))\n",
    "            find_cprate = re.findall(pat_cprate, x)\n",
    "            assert len(find_cprate) == 1\n",
    "            cprate += [float(find_cprate[0])] * num\n",
    "\n",
    "        compress_rate = cprate\n",
    "        return compress_rate\n",
    "compress_rate = process_compress_rate(compress_rate)\n",
    "print('compress_rate:' + str(compress_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T07:15:27.982440Z",
     "start_time": "2020-11-01T07:15:27.824295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.InvertedResidual'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.MobileNetV2'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "Params: 1204302.00\n",
      "Flops: 11892056.00\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.InvertedResidual'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.MobileNetV2'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "Params: 2236682.00\n",
      "Flops: 22823680.00\n"
     ]
    }
   ],
   "source": [
    "net_1 = mobilenet_v2(compress_rate=[0.] * 100).cuda()\n",
    "net_2 = mobilenet_v2(compress_rate=compress_rate).cuda()\n",
    "input_image_size=32\n",
    "input_image = torch.randn(1, 3, input_image_size, input_image_size).cuda()\n",
    "flops, params = profile(net_2, inputs=(input_image,))\n",
    "print('Params: %.2f' % (params))\n",
    "print('Flops: %.2f' % (flops))\n",
    "\n",
    "input_image_size=32\n",
    "input_image = torch.randn(1, 3, input_image_size, input_image_size).cuda()\n",
    "flops, params = profile(net_1, inputs=(input_image,))\n",
    "print('Params: %.2f' % (params))\n",
    "print('Flops: %.2f' % (flops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T13:50:08.521004Z",
     "start_time": "2020-10-20T13:50:04.255125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> Building model\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv2.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv3.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv4.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv5.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv6.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv7.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv8.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv9.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv10.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv11.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv12.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv13.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv14.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv15.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv16.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv17.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv18.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv19.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv20.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv21.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv22.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv23.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv24.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv25.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv26.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv27.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv28.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv29.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv30.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv31.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv32.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv33.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv34.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv35.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv36.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv37.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv38.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv40.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv42.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv44.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv46.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv48.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv50.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv52.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv54.npy\n",
      "-------------------\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv2.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv3.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv4.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv5.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv6.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv7.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv8.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv9.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv10.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv11.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv12.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv13.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv14.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv15.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv16.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv17.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv18.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv19.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv20.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv21.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv22.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv23.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv24.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv25.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv26.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv27.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv28.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv29.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv30.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv31.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv32.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv33.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv34.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv35.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv36.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv37.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv38.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv40.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv42.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv44.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv46.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv48.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv50.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv52.npy\n",
      "loading rank from: ./data/model/rank_conv/resnet_56_limit5/rank_conv54.npy\n"
     ]
    }
   ],
   "source": [
    "print(\"=====> Building model\")\n",
    "if arch == \"mobilenet_v2\":\n",
    "    net_1 = mobilenet_v2(compress_rate,n_class=10)\n",
    "    net_2 = mobilenet_v2(compress_rate,n_class=10)\n",
    "    net_1.to(device)\n",
    "    net_2.to(device)\n",
    "    \n",
    "    rank_conv_prefix = \"./data/model/rank_conv/mobileNetV2_limit5/\"\n",
    "    name_base=''\n",
    "\n",
    "    print('resuming from pretrain model')\n",
    "    origin_model = mobilenet_v2(compress_rate=[0.] * 100,n_class=10).cuda()\n",
    "    ckpt = torch.load(\"./data/model/PruneGraft_cifar10_MobileNetV2_preTrain/best_9.t7\")\n",
    "    # ckpt = {k:v for k,v in ckpt.items() if \"classifier\" not in k}\n",
    "    ckpt = ckpt[\"net\"]\n",
    "\n",
    "    origin_model.load_state_dict(ckpt)\n",
    "    oristate_dict = origin_model.state_dict()\n",
    "    load_mobilenetv2_model(net_1,oristate_dict)\n",
    "    load_mobilenetv2_model(net_2,oristate_dict)\n",
    "if arch == \"resnet_56\":\n",
    "    net_1 = resnet_56(compress_rate)\n",
    "    net_2 = resnet_56(compress_rate)\n",
    "    net_1.to(device)\n",
    "    net_2.to(device)\n",
    "    \n",
    "\n",
    "    rank_conv_prefix = \"./data/model/rank_conv/resnet_56_limit5/\"\n",
    "    name_base=''\n",
    "    \n",
    "    origin_model = resnet_56(compress_rate=[0.] * 100).cuda()\n",
    "    ckpt = torch.load(pretrain_dir, map_location='cuda:0')\n",
    "    origin_model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "    \n",
    "    oristate_dict = origin_model.state_dict()\n",
    "    load_resnet_model(net_1, oristate_dict, 56)\n",
    "    print(\"-------------------\")\n",
    "    load_resnet_model(net_2, oristate_dict, 56)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T13:50:09.768138Z",
     "start_time": "2020-10-20T13:50:08.522005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.ModuleList'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.LambdaLayer'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.BasicBlock'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.ResNet'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "Params: 485413.00\n",
      "Flops: 65940032.00\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.ModuleList'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.LambdaLayer'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.BasicBlock'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.ResNet'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "Params: 485413.00\n",
      "Flops: 65940032.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image_size=32\n",
    "input_image = torch.randn(1, 3, input_image_size, input_image_size).cuda()\n",
    "flops, params = profile(net_2, inputs=(input_image,))\n",
    "print('Params: %.2f' % (params))\n",
    "print('Flops: %.2f' % (flops))\n",
    "\n",
    "input_image_size=32\n",
    "input_image = torch.randn(1, 3, input_image_size, input_image_size).cuda()\n",
    "flops, params = profile(net_1, inputs=(input_image,))\n",
    "print('Params: %.2f' % (params))\n",
    "print('Flops: %.2f' % (flops))\n",
    "\n",
    "\n",
    "ck = torch.load(\"./data/model/Hrank_preTrain/cifar-10/resnet_56_prune_2_best.pth.tar\")\n",
    "net_1.load_state_dict(ck[\"state_dict\"])\n",
    "net_2.load_state_dict(ck[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T13:50:09.777147Z",
     "start_time": "2020-10-20T13:50:09.769139Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLabelSmooth(nn.Module):\n",
    "\n",
    "  def __init__(self, num_classes, epsilon):\n",
    "    super(CrossEntropyLabelSmooth, self).__init__()\n",
    "    self.num_classes = num_classes\n",
    "    self.epsilon = epsilon\n",
    "    self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, inputs, targets):\n",
    "    log_probs = self.logsoftmax(inputs)\n",
    "    targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)\n",
    "    targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
    "    loss = (-targets * log_probs).mean(0).sum()\n",
    "    return loss   \n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.cuda()\n",
    "criterion_smooth = CrossEntropyLabelSmooth(CLASSES, 0.1)\n",
    "criterion_smooth = criterion_smooth.cuda()\n",
    "\n",
    "optimizer_1 = torch.optim.SGD(net_1.parameters(), learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "optimizer_2 = torch.optim.SGD(net_2.parameters(), 0.02, momentum=momentum, weight_decay=0.006)\n",
    "lr_decay_step = list(map(int, lr_decay_step.split(',')))\n",
    "scheduler_1 = torch.optim.lr_scheduler.MultiStepLR(optimizer_1, milestones=lr_decay_step, gamma=0.1)\n",
    "scheduler_2 = torch.optim.lr_scheduler.MultiStepLR(optimizer_2, milestones=lr_decay_step, gamma=0.1)\n",
    "\n",
    "lr_scheduler_1 = torch.optim.lr_scheduler.StepLR(optimizer_1, step_size=60, gamma=0.1)\n",
    "lr_scheduler_2 = torch.optim.lr_scheduler.StepLR(optimizer_2, step_size=60, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T13:50:11.294527Z",
     "start_time": "2020-10-20T13:50:09.778147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load training data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "print('load training data')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data/cifar-10-batches-py/', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data/cifar-10-batches-py/', train=False, download=True, transform=transform_test)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T13:50:11.313544Z",
     "start_time": "2020-10-20T13:50:11.295528Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "def entropy(x, n=10):\n",
    "    x = x.reshape(-1)\n",
    "    scale = (x.max() - x.min()) / n\n",
    "    entropy = 0\n",
    "    for i in range(n):\n",
    "        p = torch.sum((x >= x.min() + i * scale) * (x < x.min() + (i + 1) * scale), dtype=torch.float) / len(x)\n",
    "        if p != 0:\n",
    "            entropy -= p * torch.log(p)\n",
    "    return float(entropy.cpu())\n",
    "def l2_norm(weight_torch):\n",
    "    weight_vec = weight_torch.view(weight_torch.size()[0],-1)\n",
    "    norm = torch.norm(weight_vec, 2, 1)\n",
    "    norm_np = norm.cpu().numpy()\n",
    "    return sum(norm_np)\n",
    "def Geometric_dist(weight_torch):\n",
    "    weight_vec = weight_torch.view(weight_torch.size()[0],-1)\n",
    "    weight_vec = weight_vec.cpu().numpy()\n",
    "    similar_matrix = distance.cdist(weight_vec, weight_vec, 'euclidean')\n",
    "    similar_sum = np.sum(np.abs(similar_matrix), axis=0)\n",
    "    similar_small_index = similar_sum.argsort()[0]\n",
    "    similar_large_index = similar_sum.argsort()[-1] \n",
    "    return similar_sum[similar_large_index]-similar_sum[similar_small_index]\n",
    "def train(epoch,i,net,optimizer,scheduler_1):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        if batch_idx % 1000 == 1000 - 1 or 1000 == trainloader.__len__() - 1:\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)' % (\n",
    "                train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "    scheduler_1.step()\n",
    "    \n",
    "def test(epoch,net,i):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    acc = 100. * correct / total\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc\n",
    "        }\n",
    "        torch.save(state, './data/model/'+save_dir+'/best_%d.t7' % (epoch))\n",
    "    print('Network:%d    epoch:%d    accuracy:%.3f    best:%.3f' % (i, epoch, acc, best_acc))\n",
    "\n",
    "def grafting(net, epoch,i):\n",
    "    while True:\n",
    "        try:\n",
    "            checkpoint = torch.load('./data/model/'+save_dir+'/ckpt%d_%d.t7' % (i - 1, epoch))['net']\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(10)\n",
    "    model = collections.OrderedDict()\n",
    "    w = 1 \n",
    "    for i, (key, u) in enumerate(net.state_dict().items()):\n",
    "        if 'conv' in key and 'weight' in key:\n",
    "            w = round(0.4 / np.pi * np.arctan(500 * (Geometric_dist(u) - Geometric_dist(checkpoint[key]))) + 0.5, 2)\n",
    "        model[key] = u * w + checkpoint[key] * (1 - w)  \n",
    "    net.load_state_dict(model)\n",
    "\n",
    "for epoch in range(200):\n",
    "    scheduler_1.step()\n",
    "    scheduler_2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T13:50:11.316546Z",
     "start_time": "2020-10-20T13:50:11.314545Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('./data/model/'+save_dir+'/ckpt%d_%d.t7' % (0, 0))['net']\n",
    "# print(net_1.state_dict()['total_ops'])\n",
    "# for i, (key, u) in enumerate(net_1.state_dict().items()):\n",
    "#     if 'conv' in key and 'weight' in key:\n",
    "#         w = round(0.4 / np.pi * np.arctan(500 * (entropy(u) - entropy(checkpoint[key]))) + 0.5, 2)\n",
    "        \n",
    "# grafting(net_1,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T13:51:18.054419Z",
     "start_time": "2020-10-20T13:50:11.317548Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network:1    epoch:0    accuracy:87.570    best:87.570\n",
      "Network:2    epoch:0    accuracy:81.720    best:87.570\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1360e2024734>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnet_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscheduler_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnet_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     state = {\n\u001b[0;32m      5\u001b[0m         \u001b[1;34m'net'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnet_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-535076051bb8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, i, net, optimizer, scheduler_1)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    train(epoch,1,net_1,optimizer_1,scheduler_1)\n",
    "    test(epoch,net_1,1)\n",
    "    state = {\n",
    "        'net': net_1.state_dict(),\n",
    "    }\n",
    "    torch.save(state, './data/model/'+save_dir+'/ckpt%d_%d.t7' % ( 1, epoch))\n",
    "    \n",
    "    train(epoch,2,net_2,optimizer_2,scheduler_2)\n",
    "    test(epoch,net_2,2)\n",
    "    state = {\n",
    "        'net': net_2.state_dict(),\n",
    "        \n",
    "    }\n",
    "    torch.save(state, './data/model/'+save_dir+'/ckpt%d_%d.t7' % ( 0, epoch))\n",
    "    \n",
    "    grafting(net_1,epoch,1)\n",
    "    grafting(net_2,epoch,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T13:51:18.055420Z",
     "start_time": "2020-10-20T13:50:03.599Z"
    }
   },
   "outputs": [],
   "source": [
    "# baseLine: lr 0.1 prune 0.3 database: cifar-10 bestAcc: 88.090\n",
    "# PG lr 0.1 prune 0.3 Acc 89.37 （lr1 != lr2\n",
    "# PG lr 0.1 same prune 0.3 Acc 90.08\n",
    "\n",
    "\n",
    "# resnet_56 \n",
    "# compress_rate = \"[0.]+[0.4]*2+[0.5]*9+[0.6]*9+[0.7]*9\" 92.72\n",
    "# [0.]+[0.4]*2+[0.5]*9+[0.6]*9+[0.7]*9  94.06\n",
    "# \"[0.]+[0.15]*2+[0.4]*27\" 93.57"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
